<xml>
<persons>
<person>
<Name>Angelos Barmpoutis</Name>
<Description><![CDATA[Angelos Barmpoutis is currently a Professor and coordinator of research and technology in the Digital Worlds Institute at the University of Florida. He is also an affiliate faculty of the Computer and Information Science and Engineering Department and the Biomedical Engineering Department, University of Florida. His current research projects focus on automated analysis of human motion, 3D reconstruction and dissemination of digital cultural heritage, applications of virtual and augmented reality, and medical image analysis. For his contribution to the aforementioned areas, he received in 2014 the Merit Award from the IEEE International Conference on Connected Vehicles, in 2016 he was finalist for the Rome Prize for Historic Preservation and Conservation, and was named UF Research Foundation Professor for 2020-2023. Dr. Barmpoutis has coauthored numerous highly cited publications in the aforementioned topics, and his work has led to patented and copyrighted inventions registered in the US, and been funded by several awards and grants from various funding agencies including the Andrew W. Mellon Foundation, the National Endowment for the Humanities, the National Institutes of Health, and the US Department of Transportation. In the international community he is also known for the Digital Epigraphy and Archaeology project that he is directing since 2011 and the highly cited Java-For-Kinect open-source library, which has been continuously used since 2013 in more than 50 countries around the world.]]></Description>
<MicrosoftAcademic>https://academic.microsoft.com/profile/72776397-00h0-48h9-e90h-j8i3h11i29e7/AngelosBarmpoutis/</MicrosoftAcademic>
<GoogleScholar>https://scholar.google.com/citations?user=XHdA6fIAAAAJ</GoogleScholar>
<Scopus><![CDATA[http://www.scopus.com/authid/detail.url?origin=AuthorProfile&authorId=15520955600]]></Scopus>
<ORCID>https://orcid.org/0000-0003-3271-7965</ORCID>
<ACMProfile>https://dl.acm.org/profile/81317490738</ACMProfile>
<IEEEProfile>https://ieeexplore.ieee.org/author/37294293100</IEEEProfile>
<URL>https://research.dwi.ufl.edu/people/angelos</URL>
</person>

<person>
<Name>Marko  Suvajdzic</Name>
<Description><![CDATA[A diverse thinker with 19+ years of achievement in academia and the creative digital research and production space. Marko’s experience includes a range of digital startups and educational projects from artificial Intelligence-intensive video game titles for major corporate clients to co-founding  5 of his own startups.
\n
\nMarko was the founder of the Computer Arts Department at the Academy of Art in Serbia, and serves as the CEO of the multi-national game studio O2D, Inc. He has lectured internationally at schools and conferences in the U.S.A., U.K., India, Serbia, China, and Norway.
\n<h4>a. Professional Preparation</h4>
\nUniversity of Arts in Belgrade, Serbia            Theory of Arts and Media                    Ph.D.   (2016)
\nAcademy of Art University, USA                     Photography                                        M.F.A. (2000)
\nUniversity of North Florida, USA                    Computer Information Systems          B.S.     (1996)
\n<h4>b. Appointments</h4>
\n2023-present  Professor, Digital Arts and Sciences, University of Florida
\n2014-2023   Associate Professor, Digital Arts and Sciences, University of Florida
\n2014-present   Associate Director, Innovation and Entrepreneurship, Digital Worlds Institute, University of Florida
\n2005-present   Co-Founder/CEO, O2D Interactive Studio, San Francisco/Belgrade
\n2013-2014       Visiting Assistant Professor, Digital Arts and Sciences, University of Florida
\n2003-2012       Department Chair, Computer Arts and Design, Academy of Art, Belgrade Serbia
\n2008-2012       Associate Professor, Advertising and Design, Academy of Art, Belgrade Serbia
\n2003-2008       Assistant Professor, Computer Arts and Design, Academy of Art, Belgrade Serbia
\n2000-2001       VP of Production, Bigprizes.com, San Francisco
\n1999-2000       Creative Director, Audiocafe.com, San Francisco]]></Description>
</person>

<person>
<Name>James Oliverio</Name>
<Description><![CDATA[James Oliverio is internationally known as a creative artist, researcher, educator, and producer, and is a frequent keynote speaker and consultant to digital media industry and education programs. He has served as Executive Director of the Digital Worlds Institute at the University of Florida since January 2001, with full professorships in Digital Arts &amp; Sciences and in Music. He holds five Emmy Awards from the Atlanta chapter of the National Academy of Television Arts and Sciences. His work has been funded by agencies including the National Endowment for the Arts, the National Science Foundation, and the Reader’s Digest Foundation. His work in internationally-distributed performing and digital arts has been featured on CNN International and the BBC.
\n
\nOliverio’s orchestral scores have been commissioned and premiered by ensembles including the Cleveland Orchestra and the symphonies of Atlanta, Columbus, and Pittsburgh, in addition to performances at Carnegie Hall and the Kennedy Center for the Performing Arts in Washington, DC. Oliverio has produced for and collaborated with Jazz @ Lincoln Center, the New York Philharmonic, the New York City Ballet and the Film Society of Lincoln Center, and served as Artistic Consultant on a number of projects with jazz legend Wynton Marsalis. Prior to becoming the Founding Director of the Digital Worlds Institute, Oliverio served as the Artist in Residence at the Georgia Institute of Technology, and also as Founding Director of AudioLab in the Graphics, Visualization and Usability Center in the Georgia Tech College of Computing.]]></Description>
</person>

<person>
<Name>Hyo jeong Kang</Name>
<Description><![CDATA[]]></Description>
</person>

<person>
<Name>Markus Santoso</Name>
<Description><![CDATA[Markus Santoso holds a Ph.D. degree from Dongseo University, South Korea. His main research interests are Augmented and Virtual Reality, Wearable Technologies, NUI, UX/UI, and Serious-game Development. He graduated in 2013 and went to Erfurt, Germany to continue his research as a Postdoctoral Fellow at the Fraunhofer IDMT under a funding scheme from the European Research Consortium for Informatics and Mathematics (ERCIM). To enter this fellowship program, Markus needed to compete with more than 300 PhDs worldwide; only 24 applications were approved. As an ERCIM fellow, Markus had the opportunity to visit other well-known European research institutes as research intern such as INRIA MimeTIC at Rennes-France with a focus on VR and at VTT in Espoo-Finland doing research in AR. After that, Markus conducted his 2nd postdoctoral with the LINDSAY Virtual Human Lab at the University of Calgary, Canada. He received an Eyes-high Postdoctoral fellowship and worked on the implementation of AR/MR for human anatomy illustration, medical education, and computational physiology. Markus also had several industrial experiences with AR/VR-related startup companies in various capacities such as Hololens Developer at Wellness Computation in Calgary, Canada and Head of Department of Licensing and Gamification Department at Octagon Studio. Starting from July 2018 Markus starts his tenure-track position at University of Florida.]]></Description>
</person>

<person>
<Name>Amelia Winger-Bearskin</Name>
<Description><![CDATA[Amelia Winger-Bearskin is an artist who innovates with artificial intelligence in ways that make a positive impact on our community and the environment. She is a<a href=https://news.ufl.edu/archive/2014/02/banks-family-commits-5-million-to-support-university-of-floridas-preeminence-i.html> Banks Family Preeminence Chair</a> and Associate Professor of <a href=https://news.ufl.edu/2020/07/nvidia-partnership/>Artificial Intelligence and the Arts</a>, at the <a href=https://digitalworlds.ufl.edu/>Digital Worlds Institute</a> at the University of Florida. She is the inventor of Honor Native Sky, a project for the U.S. Department of Arts and Culture: <a href=https://usdac.us/nativeland>Honor Native Land Initiative.</a> She founded Wampum.Codes which is both an award-winning <a href=http://wampum.codes/>podcast</a> and an <a href=https://foundation.mozilla.org/en/blog/indigenous-wisdom-model-software-design-and-development/>ethical framework for software development </a>based on indigenous values of co-creation. Wampum.codes was awarded a Mozilla Fellowship embedded at the MIT Co-Creation Studio from 2019-2020 and was featured at the 2021 <a href=https://imaginenative.org/>imagineNative </a>festival. She continued her research in 2021 at Stanford University as their artist and technologist in residence made possible by the Stanford Visiting Artist Fund in Honor of Roberta Bowman Denning (VAF) .
\n
\nIn 2019 she was an invited presenter to His Holiness, The 14th Dalai Lama, at his World Headquarters in Dharamsala for the Summit on Fostering Universal Ethics and Compassion. In 2018 she was awarded a MacArthur/Sundance Institute fellowship for her 360 video immersive installation in collaboration with the artist Wendy Red Star (supported by the Google JUMP Creator program). The non-profit she founded IDEA New Rochelle, in partnership with the New Rochelle Mayor’s Office, won the 2018 $1 Million Dollar Bloomberg Mayor’s Challenge for their VR/AR Citizen toolkit to help the community co-design their city.  In 2018 she was awarded the 100k Alternative Realities Prize for her Virtual Reality Project from Engadget and Verizon Media. Amelia is the founder of the <a href=http://www.stupidhackathon.com/>stupidhackathon.com</a>.
\n
\nAmelia is Indigenous:  Seneca-Cayuga Nation of Oklahoma, Deer Clan, Haudenosaunee (Iroquois).]]></Description>
<Linkedin>https://linkedin.com/in/ameliawb</Linkedin>
<Medium>https://studioamelia.medium.com/</Medium>
<MozillaPulse>https://www.mozillapulse.org/profile/3119/about</MozillaPulse>
<MITFellow>http://opendoclab.mit.edu/presents/amelia-winger-bearskin-fellow/</MITFellow>
<URL>http://studioamelia.com</URL>
</person>

<person>
<Name>Wenbin Guo</Name>
<Description><![CDATA[<span style=font-weight: 400;>Wenbin Guo is a Postdoc Associate at UF Digital Worlds Institute. He collaborates with an interdisciplinary team from Digital Worlds Institute, the Center of Arts in Medicine, and the Center for the Arts, Migration, and Entrepreneurship to automate analysis and annotation of human motion using artificial intelligence. He builds and runs deep learning algorithms on existing human datasets of video and skeletal sequences. He received his Ph.D. in Industrial Engineering along with a Master of Computer Science at the University of Missouri. As an independent researcher, much of his focus has been on AR/VR, human-computer interaction, healthcare, and machine learning. He developed an advanced location-based augmented reality learning environment. He was motivated to improve personnel performance in the healthcare system. He also developed a serverless conversational survey platform AI chatbot helping non-technical persons to build and deploy customized surveys using conversations.</span>]]></Description>
</person>

</persons>

<publications>

<publication>
<Name>Assessing the Effectiveness of Emoticon-Like Scripting in Computer Programming</Name>
<Description><![CDATA[<iframe src=https://www.youtube.com/embed/Lsn6bj3X8o8?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nIn this paper a new method is proposed for learning computer programming. This method utilizes a set of human-readable graphemes and tokens that interactively replace the grammatical tokens of programming languages, using a concept similar to emoticons in social media. The theoretical framework of the proposed method is discussed in detail and two implementations are presented for the programming language ECMAScript (JavaScript). The results from user testing with undergraduate students show that the proposed technique improves the student’s learning outcomes in terms of syntax recall and logic comprehension, in comparison to traditional source code editors.]]></Description>
<Author>Barmpoutis\, Angelos, Huynh\, Kim, Ariet\, Peter, Saunders\, Nick</Author>
<Journal>In Advances in Intelligent Systems and Computing 598 (Springer)\, Proceedings of the AHFE 2017 International Conference on Human Factors\, Software\, and Systems Engineering\, T. Ahram and W. Karwowski (eds.)</Journal>
<DOI>https://doi.org/10.1007/978-3-319-60011-6_7</DOI>
<Year>2017</Year>
<Pages>63-75</Pages>
<Month>9-14 July</Month>
<PDF>barmpoutis_ahfe17.pdf</PDF>
</publication>


<publication>
<Name>Reading, Writing, Lexigraphing: Active Passivity as Queer Play in Walking Simulators</Name>
<Description><![CDATA[In this article, we address the histories and influences of reading and writing within the genre of digital games called “walking simulators.” Reading is framed as an activity separate from (and, sometimes, incompatible with) the set of actions afforded to players in most game genres. Walking simulators, on the other hand, converge the act of reading and walking in complex ways that expose the playful but putatively inactive action of reading as a disruptive queering. This queering subverts the standard expectation that to count as “player” (and for walking simulatorsto count as games) one must act and produce. We call this subversion “lexigraphing,”our repurposed verb form of Garrett Stewart’s (2006) neologism “lexigraph,” which refers to paintings of written text. Lexigraphing, applied to digital games, describes the seemingly passive action of walking in a gamespace, and reading its texts, as a recursive act of writing reading. We argue that the disruptive “passivity” of lexigraphing operates as a form of queering gamespace, citing J. Jack Halberstam’s (2011) rejection of a world that is constantly doing, acting, and producing. We apply lexigraphing to walking simulators through the lens of queer game studiesas articulated by Bonnie Ruberg and Adrienne Shaw (2017), which invites us to reject limited conceptions of gamic action and participate in a more playful queering. Reading “queer”as a verb is crucial to understanding the feminist and queer actions that walking simulators welcome. With our own verb, lexigraphing, we re-articulate the active passivity of reading-as-writing in walking simulators.]]></Description>
<Author>Milligan\, Caleb Andrew, Bohunicky\, Kyle</Author>
<Journal>Press Start Journal\, Special Issue: Walking Simulators</Journal>
<Volume>5</Volume>
<Number>2</Number>
<Year>2019</Year>
<URL>https://press-start.gla.ac.uk/index.php/press-start/article/view/136/89</URL>
<Pages>51-71</Pages>
<PDF>Bohunicky_PressStart_2019.pdf</PDF>
</publication>

<publication>
<Name>Exploration of Kinesthetic Gaming for Enhancing Elementary Math Education using Culturally Responsive Teaching Methodologies</Name>
<Description><![CDATA[In this paper a novel computer-assisted culturally responsive teaching (CRT) framework is presented for teaching mathematics to 5th grade students. The curricular basis for this framework is Gloria JeanMerriex’s award winning curriculum program, which uses music and body gestures to help students build associations between mathematical concepts and culturally inspired metaphors. The proposed framework uses low-cost kinesthetic sensors along with a embodied virtual reality gamimg environment that extends such proven CRT methodologies from a traditional classroom into a digital form. A pilot study was performed to investigate the efficacy of this framework in 5th grade students. A group of 35 students participated in this study and the results are discussed in detail.]]></Description>
<Author>Barmpoutis\, Angelos, Ding\, Q., Anthony\, Lisa, Eugene\, Wanda, Suvajdzic\, Marko</Author>
<Journal><![CDATA[In Proceedings of VR16 Workshops: IEEE Virtual Reality 2016 Workshop on K-12 Embodied Learning through Virtual & Augmented Reality (KELVAR)]]></Journal>
<Month>March 19</Month>
<Year>2016</Year>
<Pages>1-4</Pages>
<DOI>https://doi.org/10.1109/KELVAR.2016.7563674</DOI>
<PDF>barmpoutis_vr16.pdf</PDF>
</publication>

<publication>
<Name>A 3D Body Posture Analysis Framework During Merging And Lane Changing Maneuvers</Name>
<Description><![CDATA[Although significant advances have been done with respect to vehicle technology and roadway construction, driver behavior remains the number one contributing factor of traffic crashes worldwide. Studies show that one of the major causes of crashes is driver inattention, which may occur when drivers are involved with secondary activities (e.g. texting, talking on the phone, or eating), and when they fail to follow the cues of the surrounding environment while driving. The objective of this study was to develop a method that monitors driver body posture and movements inside the cabin and test it among different drivers when performing merging and lane changing maneuvers, since these types of maneuvers require significant body movement and may also result in unsafe situations. The developed method was applied in a naturalistic setting where 35 drivers were invited to participate. Participants’ 3D body posture was recorded with the use of a low-cost infrared depth sensor (Microsoft Kinect). Participants’ eye gaze was also recorded with the help of an eye-tracking equipment. This paper presents analysis results of 3D body posture in conjunction with the eye tracking information during 236 merging and 287 lane changing maneuvers.]]></Description>
<Funding>US DOT/RITA through STRIDE (Project 2013-051S)\, Alabama Department of Transportation (ALDOT)\, Florida Department of Transportation (FDOT)</Funding>
<Author>Kondyli\, Alexandra, Barmpoutis\, Angelos, Sisiopiku\, Virginia, Zhang\, L., Zhao\, L., Islam\, M. M., Patil\, S. S., Hosuri\, S. Rostami</Author>
<Journal>undefined</Journal>
</publication>

<publication>
<Name>Augmented-reality environment for locomotor training in children with neurological injuries</Name>
<Description><![CDATA[In this paper a novel augmented-reality environment is presented for enhancing locomotor training. The main goal of this environment is to excite kids for walking and hence facilitate their locomotor therapy and at the same time provide the therapist with a quantitative framework for monitoring and evaluating the progress of the therapy. This paper focuses on the quantitative part of our framework, which uses a depth camera to capture the patient's body motion. More specifically, we present a model-free graph-based segmentation algorithm that detects the regions of the arms and legs in the depth frames. Then, we analyze their motion patterns in real-time by extracting various features such as the pace, length of stride, symmetry of walking pattern, and arm-leg synchronization. Several experimental results are presented that demonstrate the efficacy and robustness of the proposed methods.]]></Description>
<Funding>NIH/NCATS Clinical and Translational Science Award to the University of Florida UL1 TR000064\, and the University of Florida Informatics Institute Seed Fund Award</Funding>
<Author>Barmpoutis\, Angelos, Fox\, Emily, Elsner\, Ian, Flynn\, S.</Author>
<Journal>In LNCS 8678 (Springer) Proceedings of MICCAI14 - Workshop on Augmented Environments for Computed Assisted Interventions: (eds. C.A. Linte\, Z. Yaniv\, P. Fallavollita\, P. Abolmaesumi\, and D. R. Holmes III)</Journal>
<Month>September 14</Month>
<Year>2014</Year>
<Pages>108-117</Pages>
<URL>http://campar.in.tum.de/AECAI/WebHome</URL>
<PDF>barmpoutis_miccai14.pdf</PDF>
<DOI>https://doi.org/10.1007/978-3-319-10437-9_12</DOI>
</publication>

<publication>
<Name>Tensor Body: Real-time Reconstruction of the Human Body and Avatar Synthesis from RGB-D</Name>
<Description><![CDATA[Real-time 3D reconstruction of the human body has many applications in anthropometry, telecommunications, gaming, fashion, and other areas of human-computer interaction. In this paper a novel framework is presented for reconstructing the 3D model of the human body from a sequence of RGBD frames. The reconstruction is performed in real time while the human subject moves arbitrarily in front of the camera. The method employs a novel parameterization of cylindrical-type objects using Cartesian tensor and b-spline bases along the radial and longitudinal dimension respectively. The proposed model, dubbed tensor body, is fitted to the input data using a multistep framework that involves segmentation of the different body regions, robust filtering of the data via a dynamic histogram, and energy-based optimization with positive-definite constraints. A Riemannian metric on the space of positive-definite tensor splines is analytically defined and employed in this framework. The efficacy of the presented methods is demonstrated in several real-data experiments using the Microsoft Kinect sensor.]]></Description>
<Author>Barmpoutis\, Angelos</Author>
<Journal>IEEE Transactions on Cybernetics\, Special issue on Computer Vision for RGB-D Sensors: Kinect and Its Applications</Journal>
<Volume>43</Volume>
<Number>5</Number>
<Month>October</Month>
<Year>2013</Year>
<Pages>1347-1356</Pages>
<PDF>barmpoutis_ieeetc13.pdf</PDF>
<DOI>https://doi.org/10.1109/TCYB.2013.2276430</DOI>
</publication>

<publication>
<Name>Assessment of Haptic Interaction for Home-Based Physical Tele-Therapy using Wearable Devices and Depth Sensors</Name>
<Description><![CDATA[<ul>
\n 	<li>In this paper a prototype system is presented for home-based physical tele-therapy using a wearable device for haptic feedback. The haptic feedback is generated as a sequence of vibratory cues from 8 vibrator motors equally spaced along an elastic wearable band. The motors guide the patients’ movement as they perform a prescribed exercise routine in a way that replaces the physical therapists’ haptic guidance in an unsupervised or remotely supervised home-based therapy session. A pilot study of 25 human subjects was performed that focused on: a) testing the capability of the system to guide the users in arbitrary motion paths in the space and b) comparing the motion of the users during typical physical therapy exercises with and without haptic-based guidance. The results demonstrate the efficacy of the proposed system.</li>
\n</ul>]]></Description>
<Funding>undefined</Funding>
</publication>

<publication>
<Name>Teaching Carolingian Chant with Interactive Software: Theory, Application and Assessment</Name>
<Description><![CDATA[The science of teaching/learning and the development of interactive technology are now at a stage where an effective interactive system can be developed for the teaching and learning of the basic vocabulary and grammar of early musical notation systems. Our interdisciplinary team is developing the first such system, in addition to the first assessment tool for evaluating the effectiveness of the system. We propose to offer a presentation session that will include the following: an explanation of the software and the learning research behind it; a demonstrate of the system; an explanation of the assessment process used to determine the effectiveness of the software; an audience participation segment in which audience members will see a short demonstration video regarding a particular segment of the notation work in self-correcting, interactive exercises take an online assessment. Participants in this presentation session will gain an understanding of the benefits of interactive learning; gain an understanding of an assessment process for an interactive learning that also provides a framework for ensuring an unbiased assessment; have an experience of a new interactive software, the principals behind which could be applied to various disciplines.]]></Description>
<Author>Schaefer\, Ed, Barmpoutis\, Angelos, Tripp\, Ethan, Quincy\, S. L.</Author>
<Journal>8th Annual Conference for the Scholarship of Teaching and Learning</Journal>
<Year>2015</Year>
<Month>March 25-27</Month>
<URL>http://academics.georgiasouthern.edu/ce/conferences/sotlcommons/</URL>
</publication>

<publication>
<Name>Enhancing Global Collaboration Through Network-empowered Live Performance</Name>
<Description><![CDATA[Research and development of real-time arts performance systems has been underway at the University of Florida Digital Worlds Institute since 2001. Significant attributes of this research include the successful facilitation of synchronous global-scale performing arts events, the evolution of process and practice for arts and engineering collaborations between multi-point performance sites across the high-speed network, and the development and utilization of a unique toolkit of techniques and technologies. Examples of our global-scale networked performances include: the synchronous musical union of ethnic performers located in seven cities across five continents for 'In Common: Time' at SIGGRAPH 2005; a quartet of modern dancers located in four remote cities across Asia and North American motion-captured and mapped into a single shared Cartesian coordinate space performing on virtual percussion instruments with 3D audio in 'Same Space Same Time' (2010); the integration of multiple remote audiences providing character choices and feedback on their mobile devices (aggregated, visualized and given to the performers in real-time) during a multi-continental performance featuring network-attached Kinect devices driving synchronous representations of the distributed performers in a gaming engine for 'Icons of Innovation' at IDMAA 2012. In addition to developing the methodologies necessary to integrate various traditional and emergent technologies into these multi-faceted real-time performance systems, a number of novel techniques and collaborative relationships have resulted from this work. Using several of our distributed performances as exemplars, we will outline and then detail the esthetic, procedural, technological, and logistic considerations inherent in working with artists, engineers, and media producers across multiple time zones, cultures, and sub-nets. We have learned a considerable number of lessons that can optimize the strategic planning and implementation of distributed performing arts events, and will offer not only background and recommendations for those interested in working in this space, but also examples of the specific tools, techniques and technologies we have developed and integrated into the design and production of this work.]]></Description>
<Author>Oliverio\, James, Barmpoutis\, Angelos, Juehring\, Chad, Yudin\, Anton</Author>
<Journal>In Proceedings of the Conference on Electronic Visualization and the Arts</Journal>
<Month>July 7-9</Month>
<Year>2015</Year>
<Pages>32-39</Pages>
<DOI>https://doi.org/10.14236/ewic/eva2015.3</DOI>
</publication>

<publication>
<Name>Custom Virtual Reality System with Real-Time Therapist Interactions to Enhance Home Exercise Performance and Adherence</Name>
<Description><![CDATA[Purpose/Hypothesis:
\nFollowing lower extremity (LE) joint replacement, patients are increasingly prescribed virtual reality-based home exercise programs (HEP). One goal of virtual reality (VR) use is to promote HEP adherence. Exercise adherence, as well as exercise performance, is increased with human interaction and real-time therapist feedback, which is not commonly incorporated in commercially available VR systems. To address these limitations, a custom VR system was developed using an infrared camera for motion tracking, avatar streaming, and real-time remote therapist interactions. The primary aim of this study was to evaluate the use of this custom VR system on HEP performance in adults post LE joint replacement. We also examined patient and therapist opinions of VR system feedback features and ability to improve HEP adherence.
\nNumber of Subjects:
\n14 patients (11 female; 62.5±7.5 years) with unilateral hip (n=6) or knee (n=8) replacements (4.6±5.9 months post-surgery) and 11 therapists (6 PT, 4 OT, 1 COTA; female; &gt;2 yrs experience) participated.
\nMaterials/Methods:
\nSubjects completed two random-ordered LE exercise conditions using either the custom VR system or a conventional HEP with diagrams and written instructions while therapists observed remotely via video streaming. Four standing exercises were performed (hip flexion, abduction, extension, squats). Instructions and verbal feedback were standardized, and 3-D LE motions were recorded. Exercise performance was assessed by calculating peak joint angles and movement velocities. The effect of remote therapist interaction and verbal feedback on exercise performance during the VR condition was assessed by calculation of peak joint angles during aberrant, compensatory movements (i.e. trunk lean). Exercise performance during the two conditions was compared using paired t-tests. Patient and therapist preferences were assessed using standardized questionnaires with open-ended and Likert scale-based items.
\nResults:
\nPeak joint angles during the two conditions were not different (p&gt;.05), but movements were slower with VR use for 3 of 4 exercises (p&lt;.05) and compensations were reduced with remote therapist interactions and verbal feedback. 100% of patient and therapist participants reported preferences for remote interactions including verbal feedback and interactions with streaming avatars to display real-time movements. 79% of patients and 91% of therapists reported agreement that the VR system could improve HEP adherence.
\nConclusion:
\nA custom VR system that incorporates real-time remote therapist interactions improved HEP performance in individuals post LE joint replacement. Both patients and therapists reported high preferences for real-time interactions.
\nClinical Relevance:
\nVR systems should consider the role of real-time therapist interactions to promote engagement and adherence to HEPs, as well as provide opportunity for feedback to enhance exercise performance. Further, web-based systems can allow for multi-user group exercise sessions and engagement for those in rural locations.]]></Description>
<Funding>Institute of Informatics Seed Fund at the University of Florida</Funding>
<Author>Conroy\, Christy, Brunetti\, Gina, Freeborn\, Paul, Barmpoutis\, Angelos, Fox\,Emily</Author>
<Journal>American Physical Therapy Association Combined Sections Meeting</Journal>
<Month>February 12-15</Month>
<Year>2020</Year>
</publication>

<publication>
<Name>Digital Epigraphy Toolbox</Name>
<Description><![CDATA[<iframe src=https://www.youtube.com/embed/dt1CpBkZDNQ?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nDigital Epigraphy Toolbox is an open-source cross-platform web-application designed to facilitate the digital preservation, study, and electronic dissemination of ancient inscriptions. It allows epigraphists to digitize in 3D their epigraphic squeezes using our novel cost-effective technique, which overcomes the limitations of the current methods for digitizing epigraphic data in 2-dimensions only. The proposed toolbox contains several options for 3D visualization of inscriptions as well as a set of scientific tools for analyzing the lettering techniques and performing quantitative analysis of the letterform variations. The users will have the option to share their data or search for other uploaded collections of 3D inscriptions in a semi-supervised dynamic library. This library will be organized thematically according to language, area of origin, and date and will contain a comprehensive record of the inscription in the form of plain text, 3D model, and 2D photographs.]]></Description>
<Funding>National Endowment for the Humanities\, Office of Digital Humanities\, Grant HD‐51214‐11</Funding>
<Journal>Humanities Commons</Journal>
<Author>Barmpoutis\, Angelos</Author>
<Year>2013</Year>
<Pages>1-11</Pages>
<DOI>http://dx.doi.org/10.17613/M64W9R</DOI>
<Month>February 28</Month>
</publication>

<publication>
<Name>Applications of Virtual Environments in Experiential, STEM, and Health Science Education</Name>
<Description><![CDATA[This chapter presents examples of utilizing virtual environments for experiential learning and training purposes, with applications to several areas in the Science, Technology, Engineering &amp; Mathematics (STEM), and Health Sciences. The chapter starts with a general introduction to experiential learning, followed by a presentation of various technologies for enhancing the experience in virtual environments. The rest of the chapter is organized into two sections that discuss specific examples of virtual environments for experiential learning and therapeutic medical applications respectively. More specifically, the examples will demonstrate: the use of low-cost haptic devices in virtual environments for learning nanotechnology, experiential learning environments for forest education, the use of virtual reality theaters as an educational tool in the arts and the humanities, virtual environments for therapeutic solutions, interactive tools for treating motor disabilities using brain-computer interface for interaction with virtual environments, and experiential learning applications for microsurgical training using mixed reality and haptic feedback. The chapter concludes with a final section that discusses future research directions.]]></Description>
<Author>Barmpoutis\, Angelos, DeVane\, Benjamin, Oliverio\, James</Author>
<Journal><![CDATA[Chapter 41 In Handbook of Virtual Environments: Design\, Implementation\, and Applications\, Second Edition\, K. Hale and K. Stanney (ed.)\, CRC press\, Taylor & Francis Group]]></Journal>
<Month>July</Month>
<Year>2014</Year>
<Pages>1055-1071</Pages>
<URL>https://www.crcpress.com/Handbook-of-Virtual-Environments-Design-Implementation-and-Applications/Hale-Stanney/p/book/9781138074637</URL>
</publication>

<publication>
<Name>A Study on Visual Perturbations Effect on Balance in a VR Environment</Name>
<Description><![CDATA[Users sometimes lost their balance or even fell down when they played virtual reality (VR) games or projects. This may be attributed to degree of content, high-rate of latency, coordination of various sensory inputs, and others. The authors investigated the effect of sudden visual perturbations on human balance in VR environment. This research used the latest VR head mounted display to present visual perturbations to disturb balance. To quantify balance, measured by double-support and single-support stance, the authors measured the subject's center of pressure (COP) using a force plate. The results indicated that visual perturbations presented in virtual reality disrupted balance control in the single support condition but not in the double support condition. Results from this study can be applied to clinical research on balance and VR environment design.
\n
\nPeople live in the three-dimensional (3D) physical world and traditional two-dimensional (2D) flat images such as photo or video are lack of the third dimension information (Geng, 2014). Almost half of human brain capacity is devoted to process visual information and the limitation of flat images and 2D displays will limit human’s ability to understand the complexity of real-world objects (Geng, 2014). On the other hand, 3D display technologies improve perception and interaction with 3D scenes, and hence can make applications more effective and efficient (Mehrabi et al., 2013). Driven by the rapid improvement of computer technology, 3D display has become more powerful, affordable and comfortable. One of the 3D displays that widely adopted is stereoscopic display. Stereoscopic is recognized as one of the oldest 3D display systems and it was first proposed by C. Wheatstone in 1838. This type of display was based on stereopsis, where an observer’s left and right eyes receive different perspectives separated by a stereoscopic device that the observer is wearing (Nam Kim et al., 2013). One of the stereoscopic displays is anaglyph (Image 1a in Figure 1) that use two color filtered images and glasses that usually utilize red-cyan, red-green, green-magenta and magenta-cyan colors. The other is LC shutter system or also known as active-shutter system (Image 1b in Figure 1). It is defined as a stereoscopic technique that sends the left image to the left eye while the right eye’s view is blocked by the display device and user glasses, then presents the right image to the right eye while the left eye’s view is blocked (Turner & Hellbaum, 1986). Stereoscopic display can also use a polarized 3D system (Image 1c in Figure 1), a technique that send polarized images to the corresponding eyes through polarization glasses (Nam Kim et al., 2013).]]></Description>
<Author>Santoso\, Markus, Phillips\, David</Author>
<Journal>Cases on Immersive Virtual Reality Techniques\, Yang K (Ed.)</Journal>
<Year>2019</Year>
<Pages>67-88</Pages>
<Publisher>IGI Global</Publisher>
<DOI>http://dx.doi.org/10.4018/978-1-5225-5912-2.ch004</DOI>
</publication>

<publication>
<Name>Center of Pressure Response to Visual Perturbation in Virtual Reality</Name>
<Description><![CDATA[Human maintain their balance based on variety of sensory inputs and one of it is visual systems. In this research, authors observed the implementation of Virtual Reality (VR) technology as visual perturbations to distort healthy human’s balance. To support this research, the subject’s center of pressure (COP) was measured using force sensor. The pilot data showed a strong evidence that VR technology would work as visual perturbations to distort healthy human’s balance.]]></Description>
<Author>Santoso\, Markus, Phillips\, David</Author>
<Journal>2nd International Federation of Automatic Control Conference on Cyber-Physical and Human Systems</Journal>
<Month>December 14-15</Month>
<Year>2018</Year>
<URL>http://www.cphs2018.org/</URL>
</publication>

<publication>
<Name>‘Captain Carroll’: Camera-Movement and Device Orientation based Procedural Object Rendering Approach for Mobile VR Game</Name>
<Description><![CDATA[Mobile devices are one of the most promising platforms to bring Virtual Reality (VR) to the mass market in the present day. However, mobile device has a limited computational power compare to the personal computer (PC) meanwhile VR consume a lot of powers during its operational. In this paper, authors developed a mobile VR game, titled Captain Carroll, that employed camera-movement based procedural object rendering approach to make sure that the audience would have a seamless VR gaming experience.
\n
\nThere are about billions of sold mobile devices in the world nowadays. The latest mobile devices such as smartphone and computer tablet equipped with the latest technologies such as processor, gyroscope, accelerometer and others. Therefore, these devices are able to run a VR application. VR is defined to be a computer-generated digital environment that can be experienced and interacted with as if that environment were real [1]. VR application requires a lot of power to run it smoothly. However, due to the limited computational specifications, most devices will not be able to handle a high-quality graphic VR works. So, the overall VR quality should compromise with the device’s specification.
\n
\nSeveral approaches have been suggested and one of it is procedural rendering. Procedural generation is defined as the algorithmic creation of game content with limited or indirect user input [1]. In video games, it is used to automatically create amounts of content in game or together with one or many human players or designers. The use of this approach in game design can help with the intricate and multifarious aspects of game development; thus facilitating cost reduction [2]. Currently, the VR industries start to explore this approach to support their projects.]]></Description>
<Author>Carroll\, Lauren, O’Neil\, James, Sado\, Mark, Mabaso\, Noel, Santoso\, Markus</Author>
<Journal>IEEE International Conference on Consumer Electronics (ICCE)</Journal>
<Month>January 11-13</Month>
<Year>2019</Year>
<Pages>1-3</Pages>
<DOI>https://doi.org/10.1109/ICCE.2019.8661833</DOI>
</publication>

<publication>
<Name>Single-Support Stance and VR Implementation as Visual Perturbation in Human Balance Assessment</Name>
<Description><![CDATA[Human controls their balance through variety of sensors and one of it is visual input. Previous study found that the visual proprioceptive information is more potent than the nonvisual. In this research, authors conducted an experiment on the implementation virtual reality (VR) as visual perturbation combined with single-support stance to distort human's balance. This research focused on the single-support stance where subject stood on their dominant leg during the experiment with VR goggle.
\n
\nHuman maintain their balance to prevent fall and injury. People wasted their money for the treatments to recover from the injury caused by this factor. Falls also lead people to other dangerous or harm situation. Several sensory inputs work together to maintain human’s balance such as vestibular, proprioceptive and visual systems. In this paper, authors focused the research on the visual stimulation on human’s balance.
\n
\nPrevious study suggested that the visual proprioceptive information is more potent than the nonvisual. Any alteration in visual input can trigger different individual’s response to anticipate the environmental conditions. In 1974, Lee and Aronson conducted a research related with visual perturbation to distract human infant’s balance as shown in figure 1 [1]. In this research, they built the physical experimental room comprising three walls and a ceiling and the walls were moveable in backward and forward direction. At the beginning of experiment session, the infant subject stood in the middle of the stationary experimental room then after couple seconds the researcher moved the walls and recorded subject’s reactions.]]></Description>
<Author>Santoso\, Markus, Phillips\, David</Author>
<Journal>IEEE International Conference on Consumer Electronics (ICCE)</Journal>
<Month>January 11-13</Month>
<Year>2019</Year>
<Pages>1-4</Pages>
<DOI>https://doi.org/10.1109/ICCE.2019.8661997</DOI>
</publication>

<publication>
<Name>The Pro Strats of Healsluts: Overwatch, Sexuality, and Perverting the Mechanics of Play</Name>
<Description><![CDATA[Obey or play? In digital games, sex is often confined by medial references and mechanical impositions. Tanya Krzywinska, in “The Strange Case of the Misappearance of Sex in Video Games” (2015), explains that games often obscure representations of sex and minimize playing sex. Game series like Mass Effect (2007 – 2017) use cinematic techniques such as the fade to black to allude to the climax to come, whereas games like Playboy: The Mansion (2005) abstract sex into the act of two characters comically bouncing on each other. And when sex is featured as something for players to perform, it often emerges as a mini-game in which the play is restricted by a series of button inputs, from its origins in the 1990 bedroom of Virtual Valerie (1990) to the more recent versions seen in the God of War (2005 – 2018) games until 2010. Far from an emergent, improvisational process, sexual actions in video games feel decidedly assembled and constructed; consider the Second Life (2003) universe, where sexual positions are purchasable encoded processes within “pose balls” that repeat the same action, over and over again.
\nYet despite procedural and medial impositions designed to control sexual play in digital games, fan communities have found their own ways to erotically engage within—and outside—the game, playing out their fantasies via emergent mechanics, in-game behavior, fan art, roleplaying, and fanfiction. Building upon prior work from writers like Jenny Sundén exploring how a queer relationship within and outside World of Warcraft (2004) responded and implemented mechanics of play (2012), or Stephen Greer’s analysis of how to “play queer” within digital worlds (2013), this paper focuses on one such community surrounding the game Overwatch (2016), a competitive team-based shooter. While Overwatch has cultivated a massive fan community who are drawn to its diverse and non-normative characters, Blizzard Entertainment, the game’s developer, has been decidedly coy about the sexualities (and sexual habits) of its cast, making only a few nods via comics and other supplemental media to their love lives. As a Teen-rated cartoonish shooter meant to welcome the maximum number of consumers, little within its designed world of play would seem to acknowledge sex.]]></Description>
<Author>Bohunicky\, Kyle, Youngblood\, Jordan</Author>
<Journal>Widerscreen\, Special Issue on Sexuality and Play</Journal>
<Volume>1</Volume>
<Number>2</Number>
<Year>2019</Year>
<Pages>1-26</Pages>
<URL>http://widerscreen.fi/numerot/2019-1-2/the-pro-strats-of-healsluts-overwatch-sexuality-and-perverting-the-mechanics-of-play/</URL>
<PDF>Bohunicky_Youngblood_1_2_2019.pdf</PDF>
<ISSN>1795-6161</ISSN>
</publication>

<publication>
<Name>Ecomods: An Ecocritical Approach to Game Modification</Name>
<Description><![CDATA[As systems that model complex relationships, digital games encourage players to enact Morton's ecological thought through the actions players perform within the game world. Yet these representations, as Alenda Chang (2011) has noted, commit at least one if not all of the following missteps in their realization of in-game environments: relegating environment to background scenery, relying on stereotyped landscapes, and predicating player success on extraction and use of natural resources (58). In other words, the flora and fauna players find are often reduced to non-interactive set pieces, thereby stripping actions of the ecological and environmental impact they could have. Despite these problems, ecocritical approaches to digital games have sought to re-assert the significance of connections between game ecologies and their environmental representations by tracing cultural associations (Bianchi, Barton), and by investigating problems with materiality and waste (Apperley and Jayemanne). This essay builds on these approaches by considering “modding,” short for “modifications,” as an area for ecocritical intervention in the flattening of games' environmental representations. Specifically, this essay examines the thriving environmental modding communities around Bethesda Softworks’ The Elder Scrolls V: Skyrim. Although some of the mods released for Skyrim emphasize visual enhancements, others re-connect players to the game’s ecology and environment in meaningful ways]]></Description>
<Author>Bohunicky\, Kyle</Author>
<Journal>Ecozon@\, European Journal of Literature\, Culture and Environment: Green Computer and Video Games</Journal>
<Volume>8</Volume>
<Number>2</Number>
<Year>2017</Year>
<Pages>72-87</Pages>
<URL>http://ecozona.eu/issue/view/124</URL>
<PDF>Bohunicky_ecozona2017.pdf</PDF>
</publication>

<publication>
<Name>Assessing the Role of Virtual Reality with Passive Haptics in Music Conductor Education: A Pilot Study</Name>
<Description><![CDATA[<iframe src=https://www.youtube.com/embed/0wY5gh8elq4?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nThis paper presents a novel virtual reality system that offers immersive experiences for instrumental music conductor training. The system utilizes passive haptics that bring physical objects of interest, namely the baton and the music stand, within a virtual concert hall environment. Real-time object and finger tracking allow the users to behave naturally on a virtual stage without significant deviation from the typical performance routine of instrumental music conductors. The proposed system was tested in a pilot study (n=13) that assessed the role of passive haptics in virtual reality by comparing our proposed “smart baton” with a traditional virtual reality controller. Our findings indicate that the use of passive haptics increases the perceived level of realism and that their virtual appearance affects the perception of their physical characteristics.
\n
\nThe use of computer systems in instrumental music conductor education has been a well studied topic even outside the area of virtual reality [1]. Several systems have been proposed that offer targeted learning experiences [2,3] which may also combine gamified elements [6]. In the past decades, several visual interfaces have been designed using the available technologies at each given period of time [4,5,7], which most recently included eye tracking [8] and augmented and virtual reality platforms [3].
\n
\nRecent advances in real-time object tracking and the availability of such systems as mainstream consumer products has opened new possibilities for virtual reality applications [13, 14,]. It has been shown that the use of passive haptics in VR contribute to a sensory-rich experience [15,16], as users have now the opportunity to hold and feel the main objects of interaction within a given immersive environment, such as tools, handles, and other instruments. For example, tracking the location of a real piano can help beginners learn how to play it using virtual reality [20]. However, the use of passive haptics in virtual environments for music education is an understudied area, because it requires precise real-time tracking of objects that are significantly smaller than a piano, such as hand held musical instruments, bows, batons, etc.
\n
\nIn this paper, we present a novel system for enhancing the training of novice instrumental music conductors through a tangible virtual environment. For the purposes of the proposed system a smart baton and a smart music stand have been designed using commercially available tracking sensors (VIVE trackers). The users wear a high-fidelity virtual reality headset (HTC VIVE), which renders the environment of a virtual concert hall from the conductor’s standpoint. Within this environment, the users can feel the key objects of interaction within their reach, namely the baton, the music stand, and the floor of the stage through passive haptics. A real-time hand and finger motion tracking system continuously tracks the left hand of the user in addition to the tracking of the baton, which is usually held in the right hand. This setup creates a natural user interface that allows the conductors to perform naturally on a virtual stage, thus creating a highly immersive training experience.
\n
\nThe main goals of the proposed system are the following: a) Enhance the traditional training of novice instrumental music conductors by increasing their practice time without requiring additional space allocation or time commitment from music players, which is also cost-effective. b) Provide an interface for natural user interaction that does not deviate from the traditional environment of conducting, including the environment, the tools, and the user behavior (hand gesture, head pose, and body posture), thus making the acquired skills highly transferable to the real-life scenario. c) Just-in-time feedback is essential in any educational setting, therefore one of the goals of the proposed system is to generate quantitative feedback on the timeliness of their body movement and the corresponding music signals. d) Last but not least, the proposed system recreates the conditions of a real stage performance, which may help the users reduce stage fright within a risk-free virtual environment [9,10,11,12].
\n
\nA small scale pilot study (n=13) was performed in order to assess the proposed system and particularly the role of passive haptics in this virtual reality application. The main focus of the study was to test whether the use of passive haptics increases the perceived level of realism in comparison to a typical virtual reality controller, and whether the virtual appearance of a real physical object, such as the baton, affects the perception of its physical characteristics. These hypotheses were tested using A/B tests followed by short surveys. The statistical significance of the collected data was calculated, and the results are discussed in detail.  The reported findings support our hypotheses and set the basis for a larger-scale future study.
\n
\n<iframe src=https://www.youtube.com/embed/m8e_YHEgglo?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>]]></Description>
<Author>Barmpoutis\, Angelos, Faris\, Randi, Garcia\, Luis, Gruber\, Luis, Li\, Jingyao, Peralta\, Fray, Zhang\, Menghan</Author>
<Journal>In Proceedings of the 2020 Human-Computer Interaction International Conference\, J. Y. C. Chen and G. Fragomeni (Eds.)\, LNCS</Journal>
<Volume>12190</Volume>
<Month>July 19-24</Month>
<Year>2020</Year>
<DOI>https://doi.org/10.1007/978-3-030-49695-1_18</DOI>
<PDF>barmpoutis_HCII2020.pdf</PDF>
<Funding>College of the Arts 2020 Research Incentive Award</Funding>
<Pages>275-285</Pages>
</publication>

<publication>
<Name>Virtual Kayaking: A study on the effect of low-cost passive haptics on the user experience while exercising</Name>
<Description><![CDATA[<iframe src=https://www.youtube.com/embed/QiO9ZzyffAY?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nThis paper presents the results of a pilot study that assesses the effect of passive haptics on the user experience in virtual reality simulations of recreation and sports activities. A virtual reality kayaking environment with realistic physics simulation and water rendering was developed that allowed users to steer the kayak using natural motions. Within this environment the users experienced two different ways of paddling using: a) a pair of typical virtual reality controllers, and b) one custom-made “smart paddle” that provided the passive haptic feedback of a real paddle. The results of this pilot study indicate that the users learned faster how to steer the kayak using the paddle, which they found to be more intuitive to use and more appropriate for this application. The results also demonstrated an increase in the perceived level of enjoyment and realism of the virtual experience.
\n
\nKayaking is an outdoor activity that can be enjoyed with easy motions and with minimal skill, and can be performed on equal terms by both people who are physically able and those with disabilities [1]. For this reason, it is an ideal exercise for physical therapy and its efficacy as a rehabilitation tool has been demonstrated in several studies [1-6]. Kayaking simulations offer a minimal-risk environment, which, in addition to rehabilitation, can be used in training and recreational applications [5]. The mechanics of boat simulation in general have been well-studied and led to the design of high-fidelity simulation systems in the past decades [3,7]. These simulators immerse the users by rendering a virtual environment on a projector [1,4,6] or a computer screen that is mounted on the simulator system [2,8]. Furthermore, the users can control the simulation by imitating kayaking motions using remote controls equipped with accelerometers (such as Wii controllers) [5] or by performing the same motions in front of a kinesthetic sensor (such as Kinect sensors) [4,6].
\n
\nThe recent advances in virtual reality technologies and in particular the availability of head mounted displays as self-contained low-cost consumer devices led to the development of highly immersive virtual experiences compared to the conventional virtual reality experiences with wall projectors and computer displays. Kayaking simulations have been published as commercial game titles in these virtual reality platforms [13]. However, the use of head mounted displays in intensive physical therapy exercises bears the risk of serious injuries due to the lack of user contact with the real environment. These risks could potentially be reduced if the users maintained continuous contact with the surrounding objects such as the simulator hardware, the paddle(s), and the floor of the room, with the use of passive haptics. Additionally, the overall user experience can be improved through sensory-rich interaction with the key components of the simulated environment.
\n
\nThis paper assesses the role of passive haptics in virtual kayaking applications. Passive haptics can be implemented in virtual reality systems by tracking objects of interest in real-time and aligning them with identically shaped virtual objects, which results in a sensory-rich experience [9,10]. This alignment between real and virtual objects allows users to hold and feel the main objects of interaction including hand-held objects, tables, walls, and various tools [11,12].
\n
\nIn this paper we present a novel virtual reality kayaking application with passive haptic feedback on the key objects of interaction, namely the paddle and the kayak seat. These objects are being tracked in real-time with commercially available tracking sensors that are firmly attached to them. Although the users’ real-world view is occluded by the head-mounted display, the users can see the virtual representation of these objects and naturally feel, hold, and interact with them. Subsequently, the users can perform natural maneuvers during the virtual kayaking experience by interacting with our “smart” paddle using the same range of motions as in real kayaking.
\n
\nThe proposed system was assessed with a pilot user study (n=10) that tested the following hypotheses: a) The use of passive haptics helps users learn kayaking faster and operate the simulation better compared to the conventional controller-based interaction. b) The use of passive haptics improves the level of immersion while kayaking in virtual reality.
\n
\nThe study was undertaken at the Realities Lab of the Digital Worlds Institute at the University of Florida. The volunteers who participated in this experiment were randomly assigned to the study and the control group and experienced the proposed virtual kayaking system with and without the use of passive haptics respectively. The data collection was performed with pre- and post-test surveys. In addition, the progress of each individual user during kayaking was recorded and the collected timestamps were analyzed.
\n
\nThe results from this study are presented in detail and indicate that the use of passive haptics in this application has a statistically significant impact on the user experience and affects their enjoyment, learning progress, as well as the perceived level of realism of the virtual reality simulation.]]></Description>
<Author>Barmpoutis\, Angelos, Faris\, Randi, Garcia\, Samantha, Li\, Jingyao, Philoctete\, Joshua, Puthusseril\, Jason, Wood\, Liam, Zhang\, Menghan</Author>
<Journal>Proceedings of the 2020 HCI International Conference C. Stephanidis and M. Antona (Eds.)\, Communications in Computer and Information Science series (CCIS)</Journal>
<Month>July 19-24</Month>
<Year>2020</Year>
<Volume>1225</Volume>
<PDF>barmpoutis_HCII2020b.pdf</PDF>
<Pages>147-155</Pages>
<DOI>https://doi.org/10.1007/978-3-030-50729-9_20</DOI>
</publication>

<publication>
<Name>A Comparative Analysis of 3D User Interaction: How to Move Virtual Objects in Mixed Reality</Name>
<Description><![CDATA[This study explores three hand-interaction techniques, including the gaze and pinch, touch and grab, and worlds-in-miniature interaction. Overall, a comparative analysis reveals that the WIM provided the best usability and task performance than other studied techniques. We also conducted in-depth interviews and analyzed participants’ hand gestures. Gesture analysis reveals that shapes of furniture, as well as its perceived features such as weight, largely determined the participant’s instinctive form of hand interaction. Based on these findings, we present design suggestions that can aid 3D interaction designers to develop a natural hand interaction for mixed reality.
\n
\nThe use of hand input has received increasing attention in the virtual reality (VR) community. There is a growing body of research that has begun to cast light on hand-interaction; yet, many prior VR research predominantly focused on 3D interaction using controllers. Using bare-hands offers a distinctive user experience as opposed to using controllers. Compared to the controller where a single button is often mapped to a specific action, human hands are capable of various gestures by nature. For example, to move a virtual object, the standard design practice for VR controllers is to hold a grip button. The dynamic nature of human hands, however, makes it challenging to find and pin down the most natural hand input among possible gestures—lift, grab, push, point, pinch, and more. The fundamental difference lies in our familiarity. Unlike a controller that typically requires extra efforts of learning, we are more likely to adopt our habitual actions of using hands when interacting with the virtual object. Consequently, identifying the most natural hand interaction is a complicated process since the idea of “natural” hand interaction can vary depends on the context, and it requires an understanding of how we interact with objects in everyday life.
\n
\nPrior 3DUI research has developed the novel metaphors for hand gestures such as finger-pointing that works as a mouse cursor [10,15, 27, 39], a multimodal interaction that integrates finger-pointing and gaze [21,32,42], and multiple gestures such as grabbing and pinching for complex tasks [16, 20]. While these streams of research provide valuable insights in designing innovative hand interactions, relatively little is known on tradeoff among existing technologies. Furthermore, the majority of prior works are focused on the usability aspects of hand input. For instance, 3DUI research abounds with discussions on efficient and accurate hand inputs that address technical concerns such as object occlusions and small field of view [13,17,21,32]. The question of which hand gesture would render a natural interaction, on the other hand, remains less explored.
\n
\nTo this end, this study aims to provide a comprehensive review of the tradeoff among the well-known 3D hand interaction designs. We compare three 3D interactions; 1) multimodal interaction using gaze and pinch gesture, 2) direct touch and grab interaction, and 3) worldsin-miniature. In comparing these three interactions, we first observed participants’ behavioral responses without giving instructions. This approach helps us to examine the discoverability of each interaction design, thereby providing insights into natural hand interaction. We particularly focused on an interaction that involves selecting and arranging furniture items. Architecture and interior design are one of the most promising application areas in MR; thus findings from this study can provide immediate real-world implications.]]></Description>
<Author>Kang\, Hyo, Shin\, Jung-hye, Ponto\, Kevin</Author>
<Journal>In the Proceedings of the 2020 IEEE VR Conference</Journal>
<Year>2020</Year>
<Month>March 22-26</Month>
<DOI>https://doi.org/10.1109/VR46266.2020.00047</DOI>
</publication>

<publication>
<Name>Optical Flow, Perturbation Velocities and Postural Response In Virtual Reality</Name>
<Description><![CDATA[The purpose of this study was to investigate the effect of optical flow velocity in a virtual reality (VR) environment on user’s postural control. We hypothesized that the velocity of the optical flow will perturb user’s balance. Seventeen young, healthy participants were tested in one-foot support stances. Our study showed the visual perturbations increased COP distance and the slowest perturbation velocity induced the highest response. For VR communities, developers could use this information to raise their awareness that any sudden shift in the virtual environment at any velocity could reduce a user’s postural stability and place them at risk of falling, particularly at slower perturbation velocities.
\n
\nWatch demo here: <a href=https://www.youtube.com/watch?v=6em8CqycxTo>https://www.youtube.com/watch?v=6em8CqycxTo</a>]]></Description>
<Author>Santoso\, Markus, Phillips\, David</Author>
<Journal>In the Proceedings of the 2020 IEEE VR Conference</Journal>
<Year>2020</Year>
<Month>March 22-26</Month>
<Pages>788-789</Pages>
<DOI>https://doi.org/10.1109/VRW50115.2020.00245</DOI>
</publication>

<publication>
<Name>Discover DaVinci – A Gamified Blockchain Learning App</Name>
<Description><![CDATA[Discover DaVinci is a novel augmented reality system that incorporates blockchain technology with experiential learning to engage participants in an interactive discovery of Leonardo da Vinci’s ouvre. In the true spirit of this “Renaissance man”, Discover DaVinci explores new ideas and technologies “ahead of their time”.
\n
\nIn order to illustrate the emerging potential at the intersection of art and blockchain, we present a case study of a new interactive system produced at the University of Florida Digital Worlds Institute. 
\n
\n<iframe src=https://www.youtube.com/embed/0uKWQFqtIuA?feature=oembed width=800 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nThe technologies of mobile computing, augmented reality (AR), and blockchain are starting to merge, creating new opportunities and scenarios to interact with our environment. In AR we can look at virtual objects superimposed within a real environment and resize them, rotate them, explore and interact with them on multiple levels. With the combination of AR and blockchain, we can create a system capable of keeping track of digital assets located virtually in 3D space (i.e., spatial computing). The global scale of blockchain and related technologies heightens the potential for trade and digital distribution with a fully automated and trusted way to keep track of their creations without a “middle-man”.
\n
\nDiscover DaVinci is a novel educational tool that teaches concepts of blockchain technology through an augmented reality experiential learning game. 
\n
\nThis project was developed in collaboration with several units from the University of Florida and industry partners:
\n•	Digital Arts & Sciences Faculty (Computer Science and Digital Worlds Institute)
\n•	Digital Worlds Studios’ Artists and Programmers
\n•	Gator Blockchain Club (gatorblockchainclub.com) – Student-run blockchain club at the University of Florida
\n•	Center for Innovation and Entrepreneurship (College of Business)
\n•	Creative Campus Committee at the University of Florida
\nIndustry Partners:
\n•	DLUX, decentralized content network (dlux.io)
\n•	Steem (steem.com), and Steemit (steemit.com)
\n•	A-Frame, web VR platform (aframe.io)
\n
\nDiscover DaVinci utilizes the format of a digital, collectible trading & drafting card game with AR elements on the STEEM blockchain. Although each player “owns” their cards, all transactions are public. Every collectible card is a unique token, owned by the player - a digital asset registered to the player’s account. The aim is to draw new question cards daily, answer the questions about Leonardo DaVinci, collect the special AR invention cards, and ultimately submit the accumulated card collection into a drawing for prizes. The app was developed to honor the 500th anniversary of Leonardo Davinci by promoting new and innovative technologies. ]]></Description>
<Author>Suvajdzic\, Marko, Oliverio\, James, Barmpoutis\, Angelos, Wood\, Liam, Burgermeister\, Paul</Author>
<Journal>In the Proceedings of 2020 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)</Journal>
<Month>May 3-6</Month>
<Year>2020</Year>
<Pages>1-2</Pages>
<DOI>https://doi.org/10.1109/ICBC48266.2020.9169470</DOI>
</publication>

<publication>
<Name>Depth map of the Rosetta Stone</Name>
<Description><![CDATA[This artifact depicts the depth map of the Rosetta stone, which was algorithmically generated in 2018 as part of the Digital Rosetta Stone project. The Digital Rosetta Stone is a project developed at Leipzig University by the Chair of Digital Humanities and the Egyptological Institute/Egyptian Museum Georg Steindorff in collaboration with the British Museum and the Digital Epigraphy and Archaeology Project of the University of Florida. The aims of the project are to produce a collaborative digital edition of the Rosetta Stone, address standardization and customization issues for the scholarly community, create data that can be used by students to understand the document in terms of language and content, and produce a high-resolution 3D model of the inscription. The three versions of the text were transcribed and outputted in XML, according to the EpiDoc guidelines. Next, the versions were aligned with the Ugarit iAligner tool that supports the alignment of ancient texts with modern languages, such as English and German. All three texts were then parsed syntactically and morphologically through Treebank annotation. Finally, the project explored new 3D-digitization methodologies of the Rosetta Stone in the British Museum that enhances traditional archaeological methods and facilitates the study of the artifact. The results of this work were used in different courses in Digital Humanities, Digital Philology, and Egyptology.]]></Description>
<Author>Amin\, Miriam, Barmpoutis\, Angelos, Berti\, Monica, Bozia\, Eleni, Hensel\, Josephine, Naether\, Franziska</Author>
<Year>2018</Year>
<Month>June 28</Month>
<PDF>https://ufdc.ufl.edu/IR00011130/00001</PDF>
<DOI>http://dx.doi.org/10.17613/t1e2-0w02</DOI>
</publication>

<publication>
<Name>What is Decentralized Storytelling?</Name>
<Description><![CDATA[This piece offers highlights from a research framework called Decentralized Storytelling I developed as a Guild of Future Architects member. I continued this research as a Mozilla and Co-Creation Studio fellow in 2019–2020. We are cross-publishing this piece at the Guild of Future Architect’s publication GoFAr.
\n
\nEvery month, 55 million people play Minecraft. Discord boasts 130 million users. At any given time there are a million people watching Twitch streams. There are 1.8 billion gamers around the world.
\n
\nThese new formats are popular because they meet a deep psychological need: the basic human drive to interact with other people through stories. I call this new way of telling stories “decentralized storytelling.”
\n
\nDecentralized storytelling only seems new — many of these techniques have a precedent in much older storytelling traditions. My thinking on this method for communicating through time is heavily informed by my native tradition (I am Seneca-Cayuga, Haudenosaunee, a “‘Native New Yorker” lol). My community has practiced decentralized storytelling for generations.
\n
\nUnlike publishing, radio, film, and television, which broadcast from a single source to an audience of many, decentralized storytelling networks are peer-to-peer; they emerge from the collective space of audience participation.]]></Description>
<Author>Amelia Winger-Bearskin</Author>
<Journal>MIT Co-Creation Studio, MIT Open Doc Lab, MIT Comparative Media Studies, McArthur Foundation</Journal>
<Month>August</Month>
<Year>2021</Year>
<Pages>1-2</Pages>
<DOI>https://cocreationstudio.mit.edu/decentralized-storytelling/</DOI>
</publication>

<publication>
<Name>Improving the intensive care patient experience with virtual reality—a feasibility study</Name>
<Description><![CDATA[<div>
\n<div id=__sec1 class=sec sec-first>
\n<h3 id=__sec1title>Objectives:</h3>
\n<p id=__p2 class=p p-first-last>Patients’ stays in the ICU are often characterized by prolonged immobility, sedation, disrupted sleep, and extended periods of pain, which put ICU patients at greater risk for ICU-acquired weakness and delirium-related mortality. The aim of this study was to evaluate the feasibility and efficacy of using meditative virtual reality to improve the hospital experience of ICU patients.</p>
\n
\n</div>
\n<div id=__sec2 class=sec>
\n<h3 id=__sec2title>Design:</h3>
\n<p id=__p3 class=p p-first-last>Final report of prospective observational trial.</p>
\n
\n</div>
\n<div id=__sec3 class=sec>
\n<h3 id=__sec3title>Setting:</h3>
\n<p id=__p4 class=p p-first-last>Surgical and trauma ICUs of the University of Florida Health, an academic hospital.</p>
\n
\n</div>
\n<div id=__sec4 class=sec>
\n<h3 id=__sec4title>Patients:</h3>
\n<p id=__p5 class=p p-first-last>Fifty-nine nonintubated adult ICU patients without delirium at recruitment.</p>
\n
\n</div>
\n<div id=__sec5 class=sec>
\n<h3 id=__sec5title>Interventions:</h3>
\n<p id=__p6 class=p p-first-last>Patients were exposed to sessions of commercially available meditative virtual reality applications focused on calmness and relaxation, performed once daily for up to 7 days.</p>
\n
\n</div>
\n<div id=__sec6 class=sec>
\n<h3 id=__sec6title>Measurements and Main Results:</h3>
\n<p id=__p7 class=p p-first-last>Outcome measures included pain level, pain medication administration, anxiety, depression, sleep quality, heart rate, respiratory rate, blood pressure, delirium status, and patient ratings of the virtual reality system. Comparisons were made using paired <em>t</em> tests and mixed models. The virtual reality meditative intervention improved patients’ ICU experience with reduced levels of anxiety and depression; however, there was no evidence that virtual reality had significant effects on physiologic measures, pain, or sleep.</p>
\n
\n</div>
\n<div id=__sec7 class=sec sec-last>
\n<h3 id=__sec7title>Conclusions:</h3>
\n<p id=__p8 class=p p-first-last>The use of meditative virtual reality technology in the ICU was easily implemented and well-received by patients.</p>
\n
\n</div>
\n</div>
\n<div class=sec><strong class=kwd-title>Keywords: </strong><span class=kwd-text>anxiety, delirium, depression, intensive care unit, patient experience, virtual reality</span></div>]]></Description>
<Author>Ong\, Triton L., Ruppert\, Matthew M., Akbar\,Maisha, Rashidi\,Parisa, Ozrazgat-Baslanti\, Tezcan, Bihorac\,Azra, Suvajdzic\, Marko</Author>
<Journal>Critical Care Explorations</Journal>
<Year>2020</Year>
<Volume>2</Volume>
<Number>6</Number>
<DOI>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7314318/</DOI>
</publication>

<publication>
<Name>Developing a Patient-Centered Virtual Reality Healthcare System To Prevent the Onset of Delirium in ICU Patients.</Name>
<Description><![CDATA[Abstract—The purpose of the DREAMS project (DREAMS = Digital Rehabilitation Environment-Augmenting Medical System) is to research the feasibility and clinical potential of a virtual reality (VR) system for reducing the occurrence of delirium among patients in the intensive care unit (ICU). Preliminary results of this ongoing study show VR produces minimal clinical effects but are strongly enjoyed by patients and easy to administer. We discuss important lessons learned from applying VR in the ICU. Keywords—VR, Serious games, Health, VR therapy, ICU]]></Description>
<Author>Suvajdzic\, Marko, Bihorac\, Azra, Rashidi\, Parisa, Ruppert\, Matthew, Williams\, Seth, Ozrazgat-Baslanti\, Tezcan, Ong\, Triton, Appelbaum\, Joel</Author>
<Journal>2019 IEEE 7th International Conference on Serious Games and Applications for Health (SeGAH)</Journal>
<Year>2019</Year>
<Pages>1-7</Pages>
<DOI>https://par.nsf.gov/servlets/purl/10141582</DOI>
</publication>

<publication>
<Name>Virtual reality and human consciousness: the use of immersive environments in delirium therapy</Name>
<Description><![CDATA[<div>
\n<p id=P1 class=p p-first-last>Immersive virtual environments can produce a state of behavior referred to as “presence,” during which the individual responds to the virtual environment as if it were real. Presence can be arranged to scientifically evaluate and affect our consciousness within a controlled virtual environment. This phenomenon makes the use of virtual environments amenable to existing and in-development forms of therapy for various conditions. Delirium in the intensive care unit is one such condition for which virtual reality technology has not been evaluated to date. We are currently assessing the feasibility and utility of a delirium prevention and treatment system which implements virtual reality to improve quality of sleep, reduce pain, lower usage of sedatives, and stimulate cognition. The proposed system will consist of 3-axis wearable accelerometers, 6-DOF position trackers, a virtual reality system, and apps designed to promote sleep quality and mindfulness. Our <em>a priori</em> hypothesis is that our virtual reality therapy system would lower the occurrence of delirium in patients admitted to intensive care units.</p>
\n
\n</div>
\n<div class=sec><strong class=kwd-title>Keywords: </strong><span class=kwd-text>Virtual Reality, Serious Games, VR Therapy, Delirium, Consciousness, Games for Health, Health, Cognition</span></div>]]></Description>
<Author>Suvajdzic\, Marko, Bihorac\, Azra, Rashidi\, Parisa, Ruppert\, Matthew, Williams\, Seth, Ozrazgat-Baslanti\, Tezcan, Ong\,Triton, Appelbaum\, Joel</Author>
<Journal>Technoetic Arts</Journal>
<Year>2018</Year>
<Volume>16</Volume>
<Number>1</Number>
<Pages>75-83</Pages>
<DOI>https://www.ncbi.nlm.nih.gov/pmc/articles/pmc7571612/</DOI>
</publication>

<publication>
<Name>Blockchain art and blockchain facilitated art economy: two ways in which art and blockchain collide</Name>
<Description><![CDATA[<strong>Abstract:</strong>
\n<div>Exploring the blockchain as a subject, method, and medium, the world of art has embarked on a voyage of technological discovery unlike any other to date. So far, blockchain technology has been embraced by leaders in finance, computer sciences, transportation, bookkeeping and others, to bring efficiency, transparency, and added value to their products and services. The art world is exploring blockchain technology as well, experimenting with it as an art medium, creating art pieces that comment on it, and embracing it as a whole new way to revolutionize how art is being tracked, purchased and sold. In this paper we explore two vast categories in which art and blockchain collide today: (1) Blockchain art, and (2) Blockchain facilitated art economy.</div>]]></Description>
<Author>Suvajdzic\, Marko, Stojanovic\, Dragana, Appelbaum\, Joel</Author>
<Journal>2019 4th Technology Innovation Management and Engineering Science International Conference (TIMES-iCON)</Journal>
<Year>2019</Year>
<Pages>1-5</Pages>
<DOI>https://doi.org/10.1109/TIMES-iCON47539.2019.9024403</DOI>
</publication>

<publication>
<Name>DREAMS (Digital rehabilitation environment-altering medical system)</Name>
<Description><![CDATA[Abstract
\n<ul>
\n 	<li>Purpose: Preliminarily evaluate the feasibility and efficacy of using meditative virtual reality (VR) to improve the hospital experience of intensive care unit (ICU) patients.</li>
\n 	<li>Methods: Effects of VR were examined in a non-randomized, single-center cohort. Fifty-nine patients admitted to the surgical or trauma ICU of the University of Florida Health Shands Hospital participated. A Google Daydream headset was used to expose ICU patients to commercially available VR applications focused on calmness and relaxation (Google Spotlight Stories and RelaxVR). Sessions were conducted once daily for up to seven days. Outcome measures included pain level, anxiety, depression, medication administration, sleep quality, heart rate, respiratory rate, blood pressure, delirium status, and patient ratings of the VR system. Comparisons were made using paired t-tests and mixed models where appropriate.</li>
\n 	<li>Results: The VR meditative intervention was found to improve patients’ ICU experience with reduced levels of anxiety and depression; however, there was no evidence suggesting that VR had any significant effects on physiological measures, pain, or sleep.</li>
\n 	<li>Conclusion: The use of VR technology in the ICU was shown to be easily implemented and well-received by patients.</li>
\n</ul>]]></Description>
<Author>Suvajdzic\, Marko, Bihorac\, Azra, Rashidi\, Parisa</Author>
<Journal>2017 IEEE 5th International Conference on Serious Games and Applications for Health (SeGAH)</Journal>
<Year>2017</Year>
<Pages>1-5</Pages>
<DOI>https://arxiv.org/pdf/1906.11706</DOI>
</publication>

<publication>
<Name>Blockchain and AI in Art: A Quick Look into Contemporary Art Industries</Name>
<Description><![CDATA[<h2 id=Abs1 class=c-article-section__title js-section-title js-c-reading-companion-sections-item>Abstract</h2>
\n<div id=Abs1-content class=c-article-section__content>
\n
\nIn this exploratory text the authors review different ways in which Blockchain technology intersects with Artificial Intelligence (AI), and with art, and how it connects to a more and more frequently mentioned area such as contemporary art industries. These intersections are pointing at the two aspects worth exploring – the first one being a way in which technology (here Blockchain and AI) can be used in various fields and industries, and the other one following art as it opens its world to the new technological possibilities, enriching its forms, topics and manifestations, and questioning the status of the author as well. The art examples and case studies exhibited here will illustrate a couple of problems that can be solved and/or improved with Blockchain and AI technology. These include transparency, art data authenticity, art data monetization, smart contracts with artists, investment opportunities of NFT (non-fungible tokens), roles and activities of curators, psychology of aesthetics, and exploration of creativity.
\n
\n</div>]]></Description>
<Author>Suvajdzic\, Marko, Stojanović\, Dragana, Kanishcheva\, Iryna</Author>
<Journal>International Congress on Blockchain and Applications</Journal>
<Year>2021</Year>
<Pages>272-280</Pages>
<DOI>https://doi.org/10.1007/978-3-030-86162-9_27</DOI>
</publication>

<publication>
<Name>From Board Game to Digital Game: Designing a Mobile Game for Children to Learn About Invasive Species</Name>
<Description><![CDATA[<div class=colored-block__title>
\n<h2 id=d18505035e1 class=section__title left-bordered-title>ABSTRACT</h2>
\n</div>
\n<div class=abstractSection abstractInFull>
\n
\nInvasive species are species that cause economic and ecological harm and/or harm to human health. One challenge to managing invasive species is the lack of awareness about these species and the threats they pose. To mitigate this problem, the University of Florida Center for Aquatic and Invasive Plants developed a classroom board game for children to learn about trade-offs in managing invasive species. The game is effective in increasing knowledge about invasive species and promoting collaborative discussions. However, this board game is only accessible within the classroom. We created a mobile digital game that expands on the goals of the board game. In this paper, we discuss the design of the board and digital versions of the game, and provide some guidelines for designing digital learning games that address real-world problems that have no optimal solution, like the management of invasive species. Future work will evaluate the effectiveness of the digital game in enhancing children's knowledge about invasive species.
\n
\n</div>]]></Description>
<Author>Aloba\, Aishat, Coleman\, Gabriel, Ong\, Triton, Yan\, Shan, Albrecht\, Dehlia, Suvajdzic\, Marko, Anthony\, Lisa</Author>
<Journal>Extended Abstracts Publication of the Annual Symposium on Computer-Human Interaction in Play</Journal>
<Year>2017</Year>
<Pages>375-382</Pages>
<DOI>https://dl.acm.org/doi/10.1145/3130859.3131326</DOI>
</publication>

<publication>
<Name>A study on visual perturbations effect on balance in a VR environment</Name>
<Description><![CDATA[<h2 id=abstract class=margin-top-sm>Abstract</h2>
\nUsers sometimes lost their balance or even fell down when they played virtual reality (VR) games or projects. This may be attributed to degree of content, high-rate of latency, coordination of various sensory inputs, and others. The authors investigated the effect of sudden visual perturbations on human balance in VR environment. This research used the latest VR head mounted display to present visual perturbations to disturb balance. To quantify balance, measured by double-support and single-support stance, the authors measured the subject's center of pressure (COP) using a force plate. The results indicated that visual perturbations presented in virtual reality disrupted balance control in the single support condition but not in the double support condition. Results from this study can be applied to clinical research on balance and VR environment design.]]></Description>
<Author>Santoso\, Markus, Phillips\, David</Author>
<Journal>Cases on Immersive Virtual Reality Techniques</Journal>
<Year>2018</Year>
<DOI>https://www.igi-global.com/chapter/a-study-on-visual-perturbations-effect-on-balance-in-a-vr-environment/225123</DOI>
</publication>

<publication>
<Name>Development and Student Perception of Virtual Reality for Implant Surgery</Name>
<Description><![CDATA[<div class=custom-accordion-for-small-screen-link active>
\n<h2>Abstract</h2>
\n</div>
\n<div class=target-item custom-accordion-for-small-screen-content >
\n<div class=art-abstract in-tab hypothesis_container>(1) Introduction: New and innovative approaches to dental education have continued to improve with time. The coronavirus disease 2019 (COVID-19) pandemic forced dental education to change as social distancing implementations were enforced. Virtual reality was used as a resource before the COVID-19 pandemic, and it has become more essential due to social restrictions. Virtual reality can allow students to be fully immersed in a clinical environment without leaving their homes. (2) Methods: The development of virtual reality (VR) for implant surgery was described. Selected students filled out a survey before and after using the program. Then, a focus group discussion for the students was held to analyze the program further. (3) Results: Seven dental students enrolled in the Advanced Predoctoral Implant Program (APIP) participated in the study. Qualitative analysis of this study suggests that virtual reality can be used as a supplemental resource to enhance student learning of specific topics. Additionally, the students had positive outlooks for using virtual reality as a resource in dental education and were hopeful to use it in the future for particular topics and subjects. (4) Discussion: The advantages and disadvantages of VR application in education were described. This application allows the students to be immersed fully with virtual dental operatory. The application provides the student with an enhanced learning experience in implant dentistry. Students displayed supportive attitudes towards the applicability of VR in dental education but considered this application as an adjunctive tool for learning. (5) Conclusion: The application of this technology in dental education is promising. The use of virtual reality in teaching and learning implant dentistry offers positive enhancement, especially during these challenging times.</div>
\n</div>]]></Description>
<Author>Sukotjo\, Cortino, Schreiber\, Stephanie, Li\, Jingyao, Zhang\, Menghan, Chia-Chun Yuan\, Judy, Santoso\, Markus</Author>
<Journal>Journal of Education Sciences</Journal>
<Year>2021</Year>
<DOI>https://www.mdpi.com/2227-7102/11/4/176</DOI>
</publication>

<publication>
<Name>Sudden visual perturbations induce postural responses in a virtual reality environment</Name>
<Description><![CDATA[<div class=sectionInfo abstractSectionHeading>
\n<div class=sectionHeading>Abstract</div>
\n</div>
\n<div class=abstractSection abstractInFull>
\n
\nVirtual reality environments can be manipulated allowing controlled visual disturbances. These environmental manipulations (textures, objects, and movements) can affect postural control by creating a sensory mismatch, making it possible to investigate multisensory reweighting. VR integration with motion capture equipment is relatively new; these environmental factors have not received sufficient attention to be employed in postural related research and in VR environment development. Seventeen subjects performed 48 trials of forward and backward translational optic flow in single and double support presented on a head mounted display. Each condition was presented in three different velocities: 3 m/s, 5 m/s and 8 m/s. The effect on center of pressure distance and velocity were analyzed. A repeated measures, counterbalanced experimental design was used. Center of pressure distance travelled increased in the single support condition (<i>p</i> &lt; 0.001). The largest single support postural disturbance was observed in the slowest velocity and longest duration (3 m/s) and the smallest in the fastest velocity shortest duration (8 m/s). A slow optic flow velocity and higher duration of sudden visual translations generates greater disruption to postural control in single support. The velocity and duration of perturbations need to be carefully considered when studying human postural control and the design of VR environments.
\n
\n</div>]]></Description>
<Author>Phillips\, David, Santoso\, Markus</Author>
<Journal>Journal of Theoretical Issues in Ergonomics Science</Journal>
<Year>2021</Year>
<DOI>https://www.tandfonline.com/doi/abs/10.1080/1463922X.2020.1870052</DOI>
</publication>

<publication>
<Name>Faculty perceptions of virtual reality as an alternative solution for preclinical skills during the pandemic</Name>
<Description><![CDATA[Abstract
\n
\nThe COVID-19 pandemic has created an historic and significant disruption of education systems worldwide. Many dental institutions were forced to close and suspend their preclinical curricular and clinical activities. There are many options which comply with social distancing in reducing the transmission of diseases . The current pandemic crisis highlights the need for an alternative solution for preclinical dental education to prepare students with psychomotor skills in a safe and viable environment. This crisis has stimulated innovation in the pedagogic methodology. Virtual reality (VR)‐based simulation has been commonly used in medical education to improve the clinical skills and students’ self confidence. However, the use of virtual reality in predoctoral dental education is still limited and needs to be explored further. Both learners’ and teachers’ perspectives are critical for this pedagogy to be successful. The dental faculty's mindset and adaptation to this novel technology is essential to support this revolutionized pedagogy, particularly with the Millennial generation, and in this challenging time. Therefore, this qualitative study aimed to evaluate the dental faculty's perceptions of using VR for teaching.]]></Description>
<Author>Sukotjo\, C., Schreiber\, S., Yuan \,J. C., Santoso\, M.</Author>
<Journal>Journal of Dental Education</Journal>
<Year>2020</Year>
<DOI>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7753732/</DOI>
</publication>

<publication>
<Name>DentalVerse: Interactive Multiusers Virtual Reality Implementation to train preclinical dental student psychomotor skill</Name>
<Description><![CDATA[The COVID-19 pandemic created the largest disruption of education systems in history. Distance learning through online platforms were part of the solution. However, preclinical exercises to train psychomotor skills of learners have been challenging. The use of virtual reality (VR) in training medical students is innovative and has attracted much attention. In this study, authors presented the development of multi-user VR application for dental education. Our preliminary results showed the potential of using VR in the preclinical curriculum of dental education.
\n
\n&nbsp;]]></Description>
<Author>Crawford\, S., Hagen\, E. D., Du\, J., Husak\, E., Liao\, Z., Santoso\, M., Sukotjo\, C.</Author>
<Journal>IEEE Conference on Virtual Reality 2022</Journal>
<Month>March 12-16</Month>
<Year>2022</Year>
<DOI>https://doi.org/10.1109/VRW55335.2022.00028</DOI>
</publication>

<publication>
<Name>Immersive Learning with AI-enhanced Virtual Standardized Patient (VSP) to Improve Dental Student’s Communication Proficiencies</Name>
<Description><![CDATA[<strong>Abstract:</strong>
\nCOVID-19’s lockdown policy is causing the dental schools to halt their preclinical curricular and clinical activities, including a learning session with Standardized Satient (SP) to train student’s communication proficiencies. In this project, we developed Virtual Standardized Patient (VSP): an immersive learning with Artificial Intelligence (AI)- enhanced Virtual Standardized Patient to improve dental student’s communication proficiencies. Augmented Reality (AR) was used to immerse the virtual patient into user’s space, user also has an option to switch to Virtual Reality (VR) to fully immerse the user with the digital environment. AI element facilitates a seamless communication between virtual patient and user, and we also added an adaptive storytelling to allow student to explore several discussion’s options.]]></Description>
<Author>Gowthaman\, A., Kirova\, L., BingYu\, L., Molen\, P., Said\, I., Smith\, J., Stanbury\, A., Santoso\, M., Sukotjo\, C.</Author>
<Journal>The 14th International Conference on Advances in Computer-Human Interactions.</Journal>
<Year>2021</Year>
<DOI><![CDATA[http://www.thinkmind.org/index.php?view=article&articleid=achi_2021_3_20_20010]]></DOI>
</publication>

<publication>
<Name>Virtual Reality: Combating Dental Anxiety in the Pediatric Patient</Name>
<Description><![CDATA[To be presented at University of Illinois, College of Dentistry, Clinical and Research Day, 2022]]></Description>
<Author>Bertucci\, D., Sukotjo\, C., Yuan\, J. C., Santoso\, M.</Author>
<Journal>University of Illinois\, College of Dentistry\, Clinical and Research Day</Journal>
<Year>2021</Year>
</publication>

<publication>
<Name>Virtual Reality to Relieve Dental Anxiety: A Literature Review</Name>
<Description><![CDATA[To be presented <span style=font-weight: 400>University of Illinois, College of Dentistry, Clinical and Research Day 2022</span>]]></Description>
<Author>S. Steiner\, C. Sukotjo\, J. C. Yuan\, M. Santoso</Author>
<Journal>Virtual Reality to Relieve Dental Anxiety: A Literature Review</Journal>
<Year>2021</Year>
</publication>

<publication>
<Name>Single-Support Stance and VR Implementation as Visual Perturbation in Human Balance Assessment</Name>
<Description><![CDATA[Will be presented at <span style=font-weight: 400>The 37</span><span style=font-weight: 400>th</span><span style=font-weight: 400> IEEE International Conference on Consumer Electronics, 2022</span>]]></Description>
<Author>Santoso\, M., Phillips\, D.</Author>
<Journal>37th IEEE International Conference on Consumer Electronics</Journal>
<Year>2019</Year>
</publication>

<publication>
<Name>‘Captain Carroll’: Camera-Movement and Device Orientation based Procedural Object Rendering Approach for Mobile VR Game</Name>
<Description><![CDATA[To be presented at <span style=font-weight: 400>The 37</span><span style=font-weight: 400>th</span><span style=font-weight: 400> IEEE International Conference on Consumer Electronics</span>]]></Description>
<Author>Carroll\, L., O’Neil\, J., Sado\, M., Mabaso\, N., Santoso\, M.</Author>
<Journal>The 37th IEEE International Conference on Consumer Electronics</Journal>
<Year>2019</Year>
</publication>

<publication>
<Name>AI-driven Human Motion Classification and Analysis using Laban Movement System</Name>
<Description><![CDATA[Human movement classification and analysis are important in the research of health sciences and the arts. Laban movement analysis is an effective method to annotate human movement in dance that describes communication and expression. Technology-supported human movement analysis employs motion sensors, infrared cameras, and other wearable devices to capture critical joints of the human skeleton and facial key points. However, the aforementioned technologies are not mainstream, and the most popular form of motion capture is conventional video recording, usually from a single stationary camera. Such video recordings can be used to evaluate human movement or dance performance. Any methods that can systematically analyze and annotate these raw video footage would be of great importance to this field. Therefore, this research offers an analysis and comparison of AI-based computer vision methods that can annotate the human movement automatically. This study trained and compared four different machine learning algorithms (random forest, K neighbors, neural network, and decision tree) through supervised learning on existing video datasets of dance performances. The developed system was able to automatically produce annotation in the four dimensions (effort, space, shape, body) of Laban movement analysis. The results demonstrate accurately produced annotations in comparison to manually entered ground truth Laban annotation.]]></Description>
<Author>Guo\, W., Craig\, O., Difato\, T., Oliverio\, J., Santoso\, M., Sonke\, J., Barmpoutis\, A.</Author>
<Journal>In: Duffy\, V.G. (eds) Digital Human Modeling and Applications in Health\, Safety\, Ergonomics and Risk Management. Anthropometry\, Human Behavior\, and Communication. HCII 2022. Lecture Notes in Computer Science</Journal>
<Volume>13319</Volume>
<Year>2022</Year>
<Pages>201–210</Pages>
<DOI>https://doi.org/10.1007/978-3-031-05890-5_16</DOI>
<PDF>guo_hcii2022.pdf</PDF>
</publication>

<publication>
<Name>Design and Implementation of a New Serverless Conversational Survey System</Name>
<Description><![CDATA[<strong>Abstract:</strong>
\n<div>Conversational agents or chatbots have great potentials in improving survey responses and accessibility. However, it is still a challenge for researchers without programming skills to create voice-enabled chatbots to conduct customizable surveys. This paper presents a new easy-to-use serverless survey chatbot system based on the existing TigerAware mobile survey platform, called TigerAware chatbot. This chatbot system enables non-technical persons to build and deploy customized surveys on mobile devices as text or voice-based conversations. The system is based on Dialogflow and Firebase Realtime database, supports voice input and social dialog, and provides visual responses for users to select answers among several options. The chatbots can be deployed on any iOS and Android mobile device and platform that support Google Assistant. Survey question types that have been implemented include yes/no, multiple-choice, free response, numeric entry, date/time picker, and scale. This system is more efficient and effective than existing survey chatbot systems.</div>]]></Description>
<Author>Guo\, W., Zong\, S., Chen\, S., Zhao\, F., Shang\, Y.</Author>
<Journal>2021 IEEE International Conference on Data Science and Computer Application (ICDSCA)</Journal>
<Year>2021</Year>
<Pages>358-363</Pages>
<DOI>https://doi.org/10.1109/ICDSCA53499.2021.9650203</DOI>
</publication>

<publication>
<Name>Comparative analysis  of 3D user interaction: How to move an object in mixed reality</Name>
<Description><![CDATA[<strong>Abstract:</strong>
\n<div>Using one’s hands can be a natural and intuitive method for interacting with 3D objects in a mixed reality environment. This study explores three hand-interaction techniques, including the gaze and pinch, touch and grab, and worlds-in-miniature interaction for selecting and moving virtual furniture in the 3D scene. Overall, a comparative analysis reveals that the worlds-in-miniature provided the best usability and task performance than other studied techniques. We also conducted in-depth interviews and analyzed participants’ hand gestures in order to identify desired attributes for 3D hand interaction design. Findings from interviews suggest that, when it comes to enjoyment and discoverability, users prefer directly manipulating the virtual furniture to interacting with objects remotely or using in-direct interactions such as gaze. Another insight this study provides is the critical roles of the virtual object’s visual appearance in designing natural hand interaction. Gesture analysis reveals that shapes of furniture, as well as its perceived features such as weight, largely determined the participant’s instinctive form of hand interaction (i.e., lift, grab, push). Based on these findings, we present design suggestions that can aid 3D interaction designers to develop a natural and intuitive hand interaction for mixed reality.</div>]]></Description>
<Author>Kang, Ponto, Shin</Author>
<Journal>Proceedings of 2020 IEEE Virtual Reality and 3D User Interface (IEEE VR)</Journal>
<Year>2020</Year>
<Pages>275-284</Pages>
<DOI>doi: 10.1109/VR46266.2020.00047</DOI>
</publication>

<publication>
<Name>How 3D virtual interface can shape consumer decision: The role of informativeness and playfulness</Name>
<Description><![CDATA[<h2 class=section-title u-h3 u-margin-l-top u-margin-xs-bottom>Abstract</h2>
\n<div id=as0005>
\n<p id=sp0060>The recent rise of consumer virtual reality (VR) hardware raises important questions in the field of online marketing: what makes 3D VR more informative and playful than conventional 2D media such as a still image and a video, and how it affects the online purchase decision-making process. In this study, we mainly focus on three interface features—interactivity, visual–spatial cues, and graphics quality. We explore how each of these three interface features enhances playfulness and informativeness of shopping interface and further influences subsequent product evaluation and purchase intention. The results of the study provide two meaningful insights. First, interactivity and visual–spatial cues significantly enhance perceived informativeness and playfulness; however, the role of graphics quality was found to be more critical for 2D displays than for 3D VR environment. Second, informativeness and playfulness influence the purchase decision-making process in distinct ways. More specifically, a playful interface may enhance consumers' preference for hedonic product benefits (e.g., a stylish and attractive design), whereas informativeness is a more important explanatory variable for subsequent purchase intentions. We discuss the theoretical contribution and managerial insights the research provides for online retailers and designers.</p>
\n
\n</div>]]></Description>
<Author>Kang, Ponto, Shin</Author>
<Journal>Journal of Interactive Marketing</Journal>
<Year>2020</Year>
<Volume>49</Volume>
<Pages>70-85</Pages>
<DOI>https://doi.org/10.1016/j.intmar.2019.07.002</DOI>
</publication>

<publication>
<Name>SpArc: A VR Animating Tool at Your  Fingertips</Name>
<Description><![CDATA[<div class=colored-block__title>
\n<h2 id=d21865898e1 class=section__title left-bordered-title>ABSTRACT</h2>
\n</div>
\n<div class=abstractSection abstractInFull>
\n
\n3D animation is becoming a popular form of storytelling in many fields, bringing life to games, films, and advertising. However, the complexity of conventional 3D animation software poses steep learning curves for novices. Our work aims to lower such barriers by creating a simple yet immersive interface that users can easily interact with. Based on the focus-group interviews, we identified key functionalities in animation workflows. The resulting tool, SpArc, is designed for two-handed setups and allows users to dive into animating without complex rigging and skinning process or learning multiple menu interactions. Instead of using a conventional horizontal slider, we designed a radial time slider to reduce possible arm fatigue and enhance the accuracy of keyframe selection. The demo will showcase this interactive 3D animation tool.
\n
\n</div>]]></Description>
<Author>Li, Said, Kirova, Blokhina, Kang</Author>
<Journal>Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology (VRST)</Journal>
<Year>2021</Year>
<DOI>https://doi.org/10.1145/3489849.3489920</DOI>
</publication>

<publication>
<Name>Holokeys: Interactive Piano Education  Using Augmented Reality and IoT</Name>
<Description><![CDATA[<div class=colored-block__title>
\n<h2 id=d19460167e1 class=section__title left-bordered-title>ABSTRACT</h2>
\n</div>
\n<div class=abstractSection abstractInFull>
\n
\nThe rise of online learning poses unique challenges in music education, where live demonstration and musical synchronization are critical for student success. We present HoloKeys, a music education interface which allows instructors to play remotely located pianos using an augmented reality headset and wifi-enabled microcontrollers. This approach allows students to receive distance education which is more direct, immersive, and comprehensive than conventional video conferencing allows for. HoloKeys enables remote students to observe live instructional demonstration on a physical keyboard in their immediate environment just as they would in traditional settings. HoloKeys consists of two separate components: an augmented reality user interface and a piano playing apparatus. Our system aims to extend online music education beyond desktop platforms into the physical world, thereby addressing crucial obstacles encountered by educators and students transitioning into online education.
\n
\n</div>]]></Description>
<Author>Stanbury, Said, Kang</Author>
<Journal>Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology (VRST)</Journal>
<Year>2021</Year>
<DOI>https://doi.org/10.1145/3489849.3489921</DOI>
</publication>

<publication>
<Name>To be Defined  or Not to Be? Addressing Internal Questions in the Online Community for  Gender Diverse Youth</Name>
<Description><![CDATA[<div class=colored-block__title>
\n<h2 id=d29494502e1 class=section__title left-bordered-title>ABSTRACT</h2>
\n</div>
\n<div class=abstractSection abstractInFull>
\n
\nThe online community plays a pivotal role in the transgender community. To better understand how gender-diverse youth equip online communities during their critical stage of gender identity development, we collected online postings from TrevorSpace, one of the most active forums among gender-diverse youth in the US. While existing HCI research largely focuses on the connectedness and social aspects of online communities, our findings illustrate how trans children use online communities to explore and navigate their gender identity. The thematic analysis reveals two conflicting ideas during this exploration, a desire to figure out whether their gender identity could be defined as trans, yet not to be defined through categorical classification of trans. More specifically, we found that 1) trans youth use the community to understand the changes happening in their gender identity by sharing their experience; 2) and these experiences vary for individuals in regards to the intensity of gender dysphoria and their coping mechanisms; 3) yet, the debate centers around the meaning of trans—for example, whether gender diverse youth should experience dysphoria or physically transition to be considered trans; 4) the growing impact of trans social influencers, particularly trans Youtubers, is pointed out as one of the sources that ignite the debate surrounding the categorical classification defining trans. Altogether, our findings point to the importance of acknowledging the spectrum of the trans experience to design a more inclusive digital platform for gender-diverse youth.
\n
\n</div>]]></Description>
<Author><![CDATA[Ok\, & Kang]]></Author>
<Journal>Proceedings of Interaction Design and Children (IDC)</Journal>
<Year>2021</Year>
<Pages>552-557</Pages>
<DOI>https://doi.org/10.1145/3459990.3465200</DOI>
</publication>

<publication>
<Name>In Conversation: Decentralized Storytelling</Name>
<Description><![CDATA[<div class=book-wrapper__btn-tabs site-container>
\n<div class=book-wrapper__information site-container js-responsive-tabs r-tabs>
\n<div id=tab-1 class=tabs__panel tabs__panel--description r-tabs-panel r-tabs-state-active>
\n<div class=tabs__content>
\n
\n<strong>How to co-create—and why: the emergence of media co-creation as a concept and as a practice grounded in equity and justice.</strong>
\n
\nCo-creation is everywhere: It's how the internet was built; it generated massive prehistoric rock carvings; it powered the development of vaccines for COVID-19 in record time. Co-creation offers alternatives to the idea of the solitary author privileged by top-down media. But co-creation is easy to miss, as individuals often take credit for—and profit from—collective forms of authorship, erasing whole cultures and narratives as they do so. <em>Collective Wisdom</em> offers the first guide to co-creation as a concept and as a practice, tracing co-creation in a media-making that ranges from collaborative journalism to human–AI partnerships.
\n
\nWhy co-create—and why now? The many coauthors, drawing on a remarkable array of professional and personal experience, focus on the radical, sustained practices of co-creating media within communities and with social movements. They explore the urgent need for co-creation across disciplines and organization, and the latest methods for collaborating with nonhuman systems in biology and technology. The idea of “collective intelligence” is not new, and has been applied to such disparate phenomena as decision making by consensus and hived insects. Collective <em>wisdom</em> goes further. With conceptual explanation and practical examples, this book shows that co-creation only becomes wise when it is grounded in equity and justice.
\n
\n</div>
\n</div>
\n</div>
\n</div>
\n<div class=book-wrapper__related-books related-books site-container></div>]]></Description>
<Author>Winger-Bearskin\, Amelia</Author>
<Pages>256-269</Pages>
<Journal>In Chapter 5: Field Guide: Risks And Lessons Of Co-CreationCollective\, in book Wisdom: Co-Creating Media for Equity and Justice\, MIT press</Journal>
<Year>2022</Year>
<Month>November 1</Month>
<Publisher>MIT Press</Publisher>
<ISBN>9780262543774</ISBN>
<DOI>https://doi.org/10.7551/mitpress/13394.003.0007</DOI>
</publication>

<publication>
<Name>Prostate Capsule Segmentation in Micro-Ultrasound Images Using Deep Neural Networks</Name>
<Description><![CDATA[Prostate cancer is the most common internal malignancy among males. Micro-Ultrasound is a promising imaging modality for cancer identification and computer-assisted visualization. Identifying the prostate capsule area is essential in active surveillance monitoring and treatment planning. In this paper, we present a pilot study that assesses prostate capsule segmentation using the U-Net deep neural network framework. To the best of our knowledge, this is the first study on prostate capsule segmentation in Micro-Ultrasound images. For our study, we collected multi-frame volumes of Micro-Ultrasound images, and then expert prostate cancer surgeons annotated the capsule border manually. The lack of clear boundaries and variation of shapes between patients make the task challenging, especially for novice Micro-Ultrasound operators. In total 2099 images were collected from 8 subjects, 1296 of which were manually annotated and were split into a training set (1008), a validation set (112), and a test set from a different subject (176). The performance of the model was evaluated by calculating the Intersection over Union (IoU) between the manually annotated area of the capsule and the segmentation mask computed from the trained deep neural network. The results demonstrate high IoU values for the training set (95.05%), the validation set (93.18%) and the test set from a separate subject (85.14%). In 10-fold cross-validation, IoU was 94.25%, and accuracy was 99%, validating the robustness of the model. Our pilot study demonstrates that deep neural networks can produce reliable segmentation of the prostate capsule in Micro-Ultrasound images and pave the road for the segmentation of other anatomical structures within the capsule, which will be the subject of our future studies.]]></Description>
<Author>Guo\, Wenbin, Brisbane\, Wayne, Ashouri\, Rani, Nguyen\, Brianna, Barmpoutis\, Angelos</Author>
<Journal>20th IEEE International Symposium on Biomedical Imaging</Journal>
<Month>April 18-21</Month>
<Year>2023</Year>
<PDF>guo_isbi2023.pdf</PDF>
<Pages>1-5</Pages>
</publication>

<publication>
<Name><![CDATA[Developing Mini VR Game Engines as an Engaging Learning Method for Digital Arts & Sciences]]></Name>
<Description><![CDATA[Digital Arts and Sciences curricula have been known for combining topics of emerging technologies and artistic creativity for the professional preparation of future technical artists and other creative media professionals. One of the key challenges in such an interdisciplinary curriculum is the instruction of complex technical concepts to an audience that lacks prior computer science background. This paper discusses how developing small custom virtual and augmented reality game engines can become an effective and engaging method for teaching various fundamental technical topics from Digital Arts and Sciences curricula. Based on empirical evidence, we demonstrate examples that integrate concepts from geometry, linear algebra, and computer programming to 3D modeling, animation, and procedural art. The paper also introduces an open-source framework for implementing such a curriculum in Quest VR headsets, and we provide examples of small-scale focused exercises and learning activities.]]></Description>
<Author>Barmpoutis\, Angelos, Guo\, Wenbin, Said\, Ines</Author>
<Journal>13th IEEE Integrated STEM Education Conference</Journal>
<Month>March 11</Month>
<Year>2023</Year>
<PDF>barmpoutis_isec2023.pdf</PDF>
<Pages>1-4</Pages>
</publication>

</publications>

<projects>
<project>
<Name>Imagineering the Technosphere</Name>
<Description><![CDATA[<iframe src=https://www.youtube.com/embed/ypmaDq060oY?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nThe Imagineering and the Technosphere is a UF Intersections project funded by the Andrew W. Mellon Foundation.
\n
\nIn the face of our growing technological dependencies, our intersections group explores how humans use technology intentionally and unintentionally to alter our physical world. The group will study the accelerating pace of social technologies, such as the Internet and Artificial Intelligence.
\n
\n<iframe src=https://www.youtube.com/embed/ckNEjNj5Lps?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nThe group aims to discover what the lessons of past inventions can teach us about how to address the problems facing humanity today, particularly as they emerge in the “technosphere,” the landscape shaped by human hands. The group will develop an interactive website, host a regular research workshop, and organize events with speakers and filmmakers about technoscience. To engage students with questions of space, place, and time, a faculty member will work with students to build a mobile app for time travel in augmented reality to examine the hidden role of technology on the UF campus. This work will lay the foundation for team-taught and other new courses that will give students the tools to envision how they will “imagineer” the future of the planet while harnessing the power of technologies in environmentally and socially sustainable way.
\n
\nIn Spring 2020 our team will offer a course titled “Imagineernig the Technosphere”. The purpose of this course is to respond to the grand challenge question: “How do technologies influence our lives, then and now?” from the perspectives of our 6 thematic units: 1) Inventions and Sciences, 2) Spaces and Infrastructure, 3) Past and Future, 4) Imagining and Designing, 5) Conservation and Sustainability, 6) Culture and Society. This interdisciplinary approach will equip the students with foundational knowledge and tangible skills through weekly modules and experiential learning activities that will be organized as part of the <a href=https://research.dwi.ufl.edu/projects/technosphere/index.php/uf-quest-game/>“UF Quest Game”</a>, a gamified learning experience specially designed for this course. The students will be able to transcend the boundaries of traditional disciplines and demonstrate how the humanities serve as the foundation for understanding science and technology and how this holistic approach could affect our decision making processes in ourselves, and on a planetary scale.
\n<div class=uf18-body-copy>
\n
\n<strong>In the humanities class “Imagineering the Technosphere,” homework isn’t based on a book chapter, but an adventure through campus guided by the GPS-powered Time Traveler app.</strong>
\n
\nSisters Christine and Reyna Mae Cuales, both taking the class this semester, followed the prompts on the app, which steered them closer to their destination. So far, they’ve visited the Harn Museum of Art, the McKnight Brain Institute, the Baughman Center on Lake Alice and the <a href=https://digitalworlds.ufl.edu/institute-information/facilities/>Digital Worlds Institute in Norman Hall</a>, among others. Today, they’re closing in on a location near the historic central campus. When students successfully navigate to the mystery location using the app, a screen pops up that tells them they’ve arrived, offers some background about the place, and poses a reflection question about the place and its use over time.
\n
\nProfessor Angelos Barmpoutis says the intention of the app — and its corresponding board game — is to get students to see their surroundings in a new way.
\n
\n“These places are deeply connected to the past and tied to the future,” he said. “I’m trying to get them to think about the things they pass every day.”
\n
\nWhen students followed the app to the Norman Gym, for example, they saw a facility originally used for basketball, its wood floors still visible, hosting a weekend-long <a href=https://globalgamejam.org/2019/jam-sites/university-florida-digital-worlds-institute>video game design competition</a>. The experience gave them an opportunity to reflect on how not only the space but the nature of sports and competition evolved, Barmpoutis said.
\n
\nBarmpoutis is one of <a href=https://research.dwi.ufl.edu/projects/technosphere/Technosphere_Spring2020_SyllabusV1.2_AB.pdf>seven professors</a> who team teach the class, tackling fields that range from anthropology to historic preservation. Each professor’s lesson includes the hunt for several of the game’s 22 3D printed pieces, which students collect after finding the location and submitting video responses to the questions posed in the app.
\n
\n<iframe src=https://www.youtube.com/embed/B230Yo5SVmY?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nWalking north on Buckman Drive, the Cuales sisters can see that they’re getting closer to today’s location. Then they cross the street, and app tells them they’ve arrived. It’s Dauer Hall, an Collegiate Gothic brick building from 1936 with arches, bay windows and stained glass that once served as the student union. They record their video response about connection and continuity in historic places, then check the app for their next destination.
\n
\nReyna Mae, a pre-health student, and Christine, who’s studying sustainability and the built environment, both say they have discovered academic interests they wouldn’t have known about without the course.
\n
\n“When you’re just focused on your major, you don’t get to explore other classes,” Christine said. “Getting to know all of the professors and fields in this class opens up your eyes.”]]></Description>
<URL>https://research.dwi.ufl.edu/projects/technosphere/</URL>
<Funding>Andrew W. Mellon Foundation</Funding>
</project>

<project>
<Name>Java For Kinect (J4K)</Name>
<Description><![CDATA[<iframe src=https://www.youtube.com/embed/q0K4Y4g-hj0?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nThe J4K library is a popular open source Java library that implements a Java binding for the Microsoft's Kinect SDK. It communicates with a native Windows library, which handles the depth, color, infrared, and skeleton streams of the Kinect using the Java Native Interface (JNI).
\n
\nThe J4K library is compatible with all kinect devices (Kinect for Windows, Kinect for XBOX, new Kinect, or Kinect 2) and allows you to control multiple sensors of any type from a single application, as long as your system capabilities permit. For example you can control three Kinect 1 sensors, or one Kinect 1 and one Kinect 2 connected via USB 3.0 to the same computer. Furthermore, the J4K library contains several convenient Java classes that convert the packed depth frames, skeleton frames, and color frames received by a Kinect sensor into easy-to-use Java objects.]]></Description>
<URL>https://research.dwi.ufl.edu/projects/ufdw/j4k</URL>
</project>

<project>
<Name>Digital Epigraphy and Archaeology Project</Name>
<Description><![CDATA[<iframe src=https://www.youtube.com/embed/8_KE--_pbzE?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n
\nDEA is an interdisciplinary project initiated by scientists from the Digital Worlds Institute and the Department of Classics at the University of Florida. The goal of the project is to develop new open-access scientific tools for the Humanities and apply concepts from digital and interactive media and computer science to Archaeology and Classics. In our web-site you can view our 3D collections and interact with our on-line exhibits, read about our recent results, find interactive demos of our projects, and learn more about our future research directions. 
\n
\nBringing together Digital Media, Computer Science, and the Humanities.
\n
\n<iframe src=https://www.youtube.com/embed/yh6MyLLFSTo?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen></iframe>
\n]]></Description>
<URL>https://www.digitalepigraphy.org/</URL>
<Funding>National Endowment for the Humanities\, Office of Digital Humanities\, Grant HD‐51214‐11</Funding>
</project>

<project>
<Name>Looking for Good</Name>
<Description><![CDATA[Looking for Good (LFG) is an interdisciplinary live-stream broadcast production located in the College of the Arts that provides an interactive platform for researchers and artists to explore critical issues in the digital arts with audiences within and beyond the university. With LFG, we specifically research how live-streaming can augment and support the arts by providing a platform to create productive, safe, and democratic spaces that challenge the physical, ideological, cultural, and financial barriers that have traditionally dictated and limited who can access and be involved with the arts. Through our broadcast development, we examine how a wide range of media including 2D and 3D art, live performances, film, video games, research, etc. can be incorporated into live-streaming environments to offer compelling engagements with art, arts research, and the individuals working in these areas. In line with the college’s metanarrative, we also seek to make visible and accessible people, topics, work that falls outside of or is underrepresented in mainstream culture by focusing on LGBTQ+, critical race theory, labor, sexuality, gender, and class in the arts. With over 500 followers and growing, LFG is poised to become a significant, university-based space for democratic and open discussion of critical ideas and work within the arts.]]></Description>
<URL>https://www.twitch.tv/lfgatuf</URL>
<Twitter>https://twitter.com/lfgatuf?lang=en</Twitter>
</project>

<project>
<Name>Access Grid</Name>
<Description><![CDATA[<h4>DW Pioneers Access Grid for Cultural Projects</h4>
\nThe Access Grid (AG) is an ensemble of resources used to support human interaction across the Internet. It consists of multimedia displays, multipoint bi-directional audio, cameras, and shared spaces for presentations and interactions. It supports large-scale distributed meetings, collaborative work sessions, seminars, lectures, tutorials and training. The system is designed for group-to-group communication (differentiating it from desktop to desktop based tools that focus on individual communication). Thus, the environment enables both formal and informal group interactions. Access Grid nodes are designed spaces that explicitly contain the high-end audio and visual technology needed to provide a high-quality compelling user experience. Currently, there are one hundred fully operational sites in the US (mostly Universities) and over twenty international sites.
\n
\nThe Digital Worlds Institute has constructed four AG nodes. Locations include the Digital Worlds office, the Digital Worlds media lab, a portable node, and a conference room in the College of Fine Arts. Possible public access locations may include Phillips Center for Performing Arts, The University Auditorium, and Constans Theatre. In addition, three other campus groups have constructed their own nodes. Digital Worlds was the first group on campus to have a node operational. It is our charter to elevate the design and functionality to a new level not seen with any of the current nodes installed worldwide.
\n
\nDigital Worlds has used the AccessGrid for many distributed collaboration meetings and art performances such as Dancing Beyond Boundaries, Mask, Original Seed, Non Divisi, and Navigating Gravity.
\n
\nThe Media Monolith is a hybrid multimedia system for use both as an Access Grid node and as a system for multimedia presentations. The system is located in FAA 102. Two Pioneer 50 plasma screens are stacked vertically to fit a wall limited in width. Video cameras for the AG node are hidden in the structure next to the screens. Wireless microphones complement the Media Monolith for capturing room audio.
\n
\nThe College of Fine Arts conference room in FAA 109 also features an AccessGrid node. The display for this room is comprised of three Fujitsu 42 plasma screens tiled horizontally. Two computer controllable Sony video cameras capture people seated at the conference table and there is a stationary camera in the rear of the room. Tabletop or lapel microphones capture the voices of individual speakers.
\n
\nThe Digital Worlds media lab in CSE 413 contains a three-screen rear video projection Access Grid node. Each screen is eight feet wide by six feet in height. The two side screens can be positioned at multiple angles to the center screen at angle settings of 90, 120, and 180 degrees. The node has one video camera attached to the center screen and another video camera in the rear. Two handheld and two lapel wireless microphones are included. Audio speakers are mounted above the screens. This setup also functions as the SAGE.
\n
\nA portable Access Grid node is available. The rolling unit is configured with one video camera and four wireless microphones (two handheld and two lapel). Amplified audio speakers and at least one video projector must be supplied to use this system. At least one (three is best) duplex multicast 100 Mb network port must be available for the three computers. Three static IP addresses with DNS entries are required to configure the node.]]></Description>
</project>

<project>
<Name>Aesthetic Computing</Name>
<Description><![CDATA[<h4>UF Faculty receives grant for Aesthetic Computing</h4>
\nAesthetic Computing refers to the search for a new development of representation and notation, the exploration on the use of artistic methods and processes within common representations found in computing. A better way for people to better understand currently hard-to-follow structures for computing and mathematics.
\n
\nThree faculty, two from Computer Science, and one from English, were awarded $450,000 from the National Science Foundation to study ways in which aesthetic media and presentations can be used in creating formal models for Computer Science. The project was lead by Dr. Paul Fishwick (CISE), working with co-PIs Drs. Tim Davis (CISE) and Jane Douglas (English). With an increased efficiency for creating virtual 3D models and rapidly prototyped physical objects, computer scientists need to explore ways in which the arts can be used in the specification of software. Can software look more like a multimedia art production? The movement toward this goal has been given the name Aesthetic Computing, and it suggests a way for people to better understand currently hard-to-follow structures for computing and mathematics. The image shows a snapshot of a 3D operating system, which is a fairly large program used to control access to hardware and software resources on a computer. Examples of traditional operating systems, built with far more abstract notation and symbols, include Windows, Linux, and Mac OS. Aesthetic Computing paves the way for using personalized and culturally meaningful objects, media, and productions in the internal structures of software. This will lead to more people being able to understand and write their own programs, by building artistic productions.]]></Description>
</project>

<project>
<Name>Exciting Kids to Walk</Name>
<Description><![CDATA[<header>
\n<h3><span style=font-size: 16px;>The UF Clinical and Translation Sciences Institute Funds Interactive Rehabilitation Environment</span></h3>
\n</header>
\n<div class=main>
\n
\nThe UF Clinical and Translation Sciences Institute awards a $7,500 grant to support a pilot project for promoting walking recovery and enhancing the sensory input in kids with spinal cord injuries. Digital Worlds Professor Angelos Barmpoutis is participating as a co-investigator in this project along with other professors from the UF departments of Neuroscience and Physical Therapy. The goal of this collaborative team, lead by Dr. Emily J. Fox, is the development of a game-type environment to motivate disabled kids to walk.
\n
\nThe incorporation of interactive games and virtual reality (VR) is an innovative approach for making rehabilitation more engaging. Game technology motivates children, promotes practice, and performance of specific motor skills. Although games have demonstrated therapeutic effects when applied to children with neurological injuries, most games are not designed with consideration to motor impairments or for use in the LT environment. Therefore, the long-term objective in this project is to develop interactive gaming technology for the advancement of locomotor training interventions for children with neurological injuries. This project is the first phase in meeting this objective and will result in the development of a technical game prototype. A collaborative multidisciplinary team has been formed with expertise in basic neuroscience, rehabilitation, computer science, and game development. Focus groups of 8 children with SCI and CP, along with their caregivers and clinicians (Physical Therapists, Physiatrists) will be formed. Feedback from these groups will be incorporated into the team’s development of a game-design document. Using an iterative game development approach, a game software prototype will be developed and optimized for use with LT. Recently- released PrimeSense™ technology that allows for interactive controller-free play will be used and interfaced with the game prototype. Development of this game prototype and pilot data from its use will lead to a competitive NIH grant application. Moreover, the combined application of basic science, rehabilitation, and game-technology has a high likelihood of enhancing walking rehabilitation approaches for children with neurological injuries.
\n
\n</div>]]></Description>
<Funding>UF Clinical and Translational Science Institute</Funding>
</project>

<project>
<Name>Biomedical Engineering Building Pre-Visualization</Name>
<Description><![CDATA[[embed]https://vimeo.com/44736778[/embed]]]></Description>
</project>

<project>
<Name>College of Education Building Pre-Visualization</Name>
<Description><![CDATA[[embed]https://vimeo.com/44740488[/embed]]]></Description>
</project>

<project>
<Name>Dream Machine</Name>
<Description><![CDATA[As a tribute to the rich 150-year legacy of the University of Florida, Digital Worlds presented an original digital movie entitled DREAM MACHINE. This presentation was produced by the very first graduating classes of the new Digital Arts and Sciences (DAS) program, a partnership founded in the UF Colleges of Engineering and Arts.
\n
\nDREAM MACHINE was an interdisciplinary collaboration from a team of over 100 UF students with backgrounds in art, animation, architecture, engineering, theater, English, journalism, computer modeling, music, photography, film making and liberal arts and sciences. It was particularly appropriate that members of the DA's Class of 2003 honored their Alma Mater by creating a work that embodies the spirit of the UF Sesquicentennial: Honoring the Past, Shaping the Future.]]></Description>
</project>

<project>
<Name>Exploring Ancient Egypt</Name>
<Description><![CDATA[The Digital Worlds Institute faculty collaborated on the creation of a new interactive experience for young museum visitors entitled <em>Exploring Ancient Egypt.</em> Working across disciplines, including computer programming, 2D and 3D art and animation, music and sound design and interactive narrative, the DW faculty team created the original interactive experience to allow up to four simultaneous players to explore various scenarios based on themes inspired by ancient Egypt. It recently debuted at the South Florida Science Center in West Palm Beach.
\n
\nDW computer scientist Angelos Barmpoutis created original code that allows an array of two Microsoft Kinects to bring 4 separate players into one shared virtual environment through the progression of the game. The design team of Diana Reichenbach and Hyuk Jang provided the visual elements to bring the ancient world to life. Game producer Marko Suvajdzic offered consultation on incorporating gameplay elements to keep the participants actively engaged throughout the four scenarios. Patrick Pagano provided research and audio elements to inform the final sound design, and composer James Oliverio provided the soundtrack for both the title sequence and each of the four separate scenarios.
\n
\nAs the Art Director for the project, Reichenbach worked closely with Digital Worlds graduate research assistants to create an aesthetically pleasing and balanced user interface, which was a unique challenge for a game with four physical players present on one screen.
\n
\n“This project is a great example of the Institute’s capabilities, as it incorporates elements of both aesthetic and programmatic design through collaboration between all Digital Worlds faculty and its graduate research assistants,” Reichenbach said.
\n
\nReichenbach designed the backdrops to the four levels of the game, including <em>Travel the Nile</em>,<em>Build the Pyramid</em>, <em>Wrap the Mummy</em> and <em>Dance Egyptian Style</em>, and paid close attention to recreating real-life Egyptian scenery into each level.
\n
\nHyuk Jang was in charge of character development for the game. Jang’s creative process began with using various references to explore details like outfits, hairstyles and even eye color, to get a better idea of how he should develop each character, and make them more believable.
\n
\nThe next step, character modeling, was made possible with 3D software named Maya.
\n
\n“Maya is one of the industry standard software that helps us to create organic or any kinds of 3D objects and characters for games and animation,” Jang said.
\n
\nJang began with a box and sculpted out the final model.
\n
\n“This was a time consuming part because sometimes I had to spend hours on small details,” Jang explained.
\n
\nOnce the model was approved, he started to paint the textures that were placed onto the characters.
\n
\nRigging, the process of adding bones to the characters, was the final step of character development for the game. The Xbox Kinect detects the figure primarily where joints are, and sends that data to the character bones in real-time, thus creating its final form.
\n
\nReichenbach and Jang recently appeared on LiveVibeTV’s Season Four show “Opening Minds: People Who Make Video Games” to give an overview of the design process.
\n
\nUpon completion, <em>Exploring Ancient Egypt</em> was installed at the South Florida Science Center’s <em>Afterlife: Tombs and Treasures of Ancient Egypt </em>exhibit that explores the ancient Egyptian concept of the afterlife including over 200 authentic artifacts. The centerpiece of this award-winning exhibit features a full-size reproduction of the burial chamber of Thutmose III (1490-1436 B.C.), a Ramesside male mummy believed to be the son of Ramses II and several other mummies. Other unique highlights of this exhibit include 3,000 year-old artifacts including fine jewelry, painted reliefs, implements used in religious rituals, coffins and more. The exhibit will run until April 18, 2015.]]></Description>
</project>

<project>
<Name>Gaming Against Plagiarism</Name>
<Description><![CDATA[<iframe src=https://player.vimeo.com/video/64391791 width=1200 height=600 frameborder=0 allow=autoplay; fullscreen allowfullscreen></iframe>
\n
\n<h4>Synopsis of the Gaming Against Plagiarism (GAP) series</h4>
\n<strong>Game 1: Cheats and Geeks</strong>
\nInspired by the board game Candy Land, the player has to navigate through Peer Reviews, hidden Quizzes about plagiarism, and Funding Opportunities as they race to get the first publication in their cohort. Because of the prevailing lax attitudes toward plagiarism on campus, the player is able to try to cheat their way across the game board to victory. The player can cheat (A) by plagiarizing the position of the other player ahead of them, (B) by falsifying their position on the game board to advance, or (C) fabricating a path on the game board that does not exist.
\n
\n<strong>Game 2</strong>
\nThe player goes to work on behalf of the anti-plagiarism corps combating research misconduct around campus. However, they need boots on the ground. In this hybrid management/matching game that builds on the SimCity model, the player navigates a graphical map of a college campus, rushing to and from buildings that have a research misconduct problem. Once at a building where a research misconduct conflict is happening, a player has to identify the type of research misconduct from six types of plagiarism as well as falsification and fabrication, and implement interventions to detect, prevent or resolve issues of research misconduct.
\n
\n<strong>Game 3</strong>
\nTo quash the research misconduct problem on campus and return it to normal, the player is tasked with catching an arch-plagiarist in the act. The player acts as a detective, sleuthing through clues and interviewing suspects and witnesses as she tries to discover who the arch-plagiarist is. The player has to construct proofs using an advanced “argumentation constructor interface“ about what actually constitutes plagiarism, thereby exhibiting their ability to analyze and evaluate different types of plagiarism.
\n
\n]]></Description>
<Funding>National Science Foundation Grant #1033002</Funding>
<URL>http://digitalworlds.ufl.edu/gap/</URL>
<FundingURL>https://www.nsf.gov/awardsearch/showAward?AWD_ID=1033002</FundingURL>
</project>
<project>
<Name>Gator Nation Island</Name>
<Description><![CDATA[The Digital Worlds Institute developed Gator Nation Island in the Second Life online world. <a href=https://digitalworlds.ufl.edu/site/assets/files/91828/slstoryingovvideo.pdf>Read More</a>]]></Description>
</project>

<project>
<Name>Grand Guard</Name>
<Description><![CDATA[<h4>Grand Guard, Portraits in Digital Media</h4>
\n<em>University of Florida Alums </em><em>return</em><em> - 50 years after graduation.</em>
\n
\nAs a tribute to the UF graduates of the post WWII era, Digital Worlds (DW) and students in the Digital Arts and Sciences (DAS) program partnered with the UF Foundation to create original digital movies. Alumni from the graduating classes of 1952 and 1953 shared their college memories and love for their Alma Mater, and UF students from the graduating classes of 2003 and 2004 created original works of digital media to honor their forebears. The movies premiered at special banquets on the UF campus when the Alums returned to be inducted into the ranks of the Grand Guard, a prestigious element of the UF community.
\n
\nProject principals included Adam Portnow (video editor 2002 and '03), Eric Wilson (computer animation '02), Andy Quay (technical director 2002 and '03), Arturo Sinclair (director and animator 2003), Joella Walz (producer 2003) and James Oliverio (executive producer 2002 and '03).]]></Description>
</project>

<project>
<Name>Use of Haptics in a Virtual Reality Environment for Learning of Nanotechnology</Name>
<Description><![CDATA[[embed]https://vimeo.com/64391663[/embed]
\n
\nHAPNAN<strong> </strong>is an innovative and exciting educational tool geared toward middle school students. It is a collaborative effort between the University of Florida (UF) Digital Worlds Institute and UF professor of Mechanical and Aerospace Engineering and Nanotechnology expert, Dr. Curtis Taylor.  HapNan involves the merging of nanotechnology with haptics. The HapNan project incorporates the use of a haptic mouse and virtual reality computer program to create an interactive learning environment that has the potential to revolutionize the way that science is being taught within a classroom setting.  Students who are able to work with the HapNan technology will be able to explore many abstract scientific concepts in a whole new way, through sight and touch. The Haptic mouse will allow students to actually feel surface textures of the objects they are seeing on the computer screen. Additionally, the new low-cost haptic mouse allows users to actually feel invisible forces (such as gravity, attraction, repulsion and resistance) thus making these concepts tangible. HapNan allows young students to more fully understand concepts such as the covalent bonds within a water molecule and the relationship between gravity and inertia at a variety of physical scales.
\n
\nBuilt into the program is a quiz system that will allow individual instructors the ability to gauge and assess how much information each student is retaining after exploring each level. HAPNAN’s built-in lesson plans will be geared toward specific parameters and approved curriculum, giving it the potential to be an influential tool within any classroom and significantly changing the way in which science education is approached.
\n
\nHAPNAN is currently still in the developmental phase, with progress being made every day in refining the Haptic systems as well as improving the user interface. User testing is slated to begin in late Spring 2012.]]></Description>
<Funding>NSF Award #0935131\, COLLABORATIVE PROPOSAL: Use of Haptics in a Virtual Reality Environment for Learning of Nanotechnology</Funding>
<FundingURL>https://www.nsf.gov/awardsearch/showAward?AWD_ID=0935131</FundingURL>
</project>

<project>
<Name>Human Interactive Simulation and Training</Name>
<Description><![CDATA[The Human Interactive Simulation and Training (HIST) system allowed larger numbers of medical students to interact with patient symptoms, both in the local classroom and at a distance. The system tracked each student’s reactions to dynamic medical situations, giving professors a useful tool to assess individual progress and understanding.
\n
\nThe Human Patient Simulator (HPS), previously developed by the Department of Anesthesiology in the UF College of Medicine, was an ideal learning tool for anesthesiologists and other health-care providers. With the HPS, computers controlled a mannequin patient to respond to injected medications, changes in mechanical ventilation, and other therapies.
\n
\nHowever, high demand and hardware costs for the Human Patient Simulator prohibited timely and frequent access by large numbers of medical students. In response to this situation, UF anesthesiology professors Tammy Euliano and Nik Gravenstein asked the Digital Worlds Institute to collaborate in creating a system that would augment access to the HPS in both the classroom and at remote learning sites. Digital Worlds engineer Andy Quay teamed with digital artist Arturo Sinclair to create the Human Interactive Simulation Training (HIST) system.
\n
\nAt the end of the simulated procedure, the professor was able to review the steps taken by each student, help assess their thought processes, and suggest ways to reduce mistakes and time spent on their assessments. Studies at UF (1) indicate that medical students “acquire and retain skills better” using the interactive simulators than they do through lectures or other passive tools like books or videotapes.
\n
\nRegarding the new HIST system, Dr Gravenstein said “The department of Anesthesiology of the University of Florida makes use of 'Mental Models' when teaching students and young physicians the intricacies of the cardiovascular system. One such Mental Model was developed in cooperation between anesthesiologists and members of DW. The new model will see almost daily use and will be presented this summer at an international meeting in Europe”.
\n
\nDW Associate Director Andy Quay stated that “…the HIST project is an excellent example of how digital artists and media systems engineering can provide tangible benefits to fields ranging from medicine to aerospace engineering and beyond. We are pleased with this initial collaboration with UF anesthesiologists and look forward to expanding our research and development with medical scientist and educators”.]]></Description>
</project>

<project>
<Name>Integrated Situational Awareness System</Name>
<Description><![CDATA[[embed]https://vimeo.com/44671745[/embed]
\n
\nThe Integrated Situation Awareness System enhances decision-making effectiveness when managing potentially life-threatening situations. Researchers at Digital Worlds have designed a system that combines a variety of inputs, including live video feeds, federated databases, security systems, and satellite imagery, simultaneously, to augment the decision maker's situational awareness. Essentially, this ISAS is designed for public safety, wide-area environmental monitoring, law-enforcement and anti-terror purposes.
\n
\nIn a complex and rapidly-changing situation, decision-makers need constant access to timely, accurate, and comprehensible information. To bring a situation to the desired resolution in the shortest possible time, resources must be deployed strategically as critical decisions are being made. The Integrated Situational Awareness System (ISAS) combines a wide variety of inputs, including live video feeds, databases, security systems and satellite imagery simultaneously, to augment the decision-makers' situational awareness.
\n
\nThe Officer in the field uses an augmented vision system, to share real-time information with the ISAS Operator at the remote command center. The Operator then supplies the Decision Maker with real-time data as requested. The Decision Maker makes swift correlations and is better able to direct the dynamic relationship between resources and desired outcomes. The Officer is accompanied by 4 Micro Aerial Vehicles called a Halo. The Halo can be controlled by the officer or by the ISAS Operator. The Operator can also control a larger group of MAVs called a Swarm. Each MAV in the Swarm is able to carry out a variety of sensing functions to help augment the Decision Makers' decisional awareness.]]></Description>
</project>

<project>
<Name>International Media Union</Name>
<Description><![CDATA[[embed]https://vimeo.com/44743065[/embed]]]></Description>
</project>

<project>
<Name>Legends of Antiquity</Name>
<Description><![CDATA[]]></Description>
</project>

<project>
<Name>Mask</Name>
<Description><![CDATA[<h4>MASK, Dance and Visual Art Collaboration, U. de Chile - REUNA - and Digital Worlds</h4>
\nA dance and visual art collaboration over the Internet connected The University of Chile REUNA and Digital Worlds. On May 17, 2004 the University of Florida's Digital Worlds Institute collaborated with the <a href=http://www.reuna.cl/ target=_blank rel=noopener noreferrer>Universitaria Nacional (REUNA)</a> in Santiago de Chile and the New World School of the Arts (NWSA) in Miami for a three-minute intercontinental collaborative dance performance. This experience involved four dancers, one couple in Chile (dancing live in an auditorium during the opening ceremony of the fourth International Meeting Science, Culture and Education on the Research and Development Network) and the other couple in the USA, performing a collaborative choreography, using chroma key technology and MPEG2 over IP running in real time over the Internet2 network.]]></Description>
</project>

<project>
<Name>Micro-Air-Vehicle Environment System</Name>
<Description><![CDATA[Micro-Air-Vehicle Virtual Environment System, A Hardware-in-the-Loop Experiment and Simulation Facility for Vision-Based Control of Micro-Air-Vehicles (MAVs)
\n
\nThe UF Micro-Air-Vehicle (MAV) Virtual Environment systems provide a synthetic environment in which hardware MAVs can be virtually flight tested. The project, headed by Dr. Andrew Kurdila and Dr. Rick Lind of the UF Mechanical and Aerospace Engineering Department, is funded by the United States Air Force.
\n
\nThe facilities operate by measuring physical response of MAVs in a wind tunnel, estimating the MAVs’ inertial location in a virtual urban environment, flying the MAV in this environment, and viewing the virtual urban terrain in real-time. The two compatible research facilities enable the study of:
\n
\n1- Vision-processing algorithms for real-time identification of critical features required for coordinated guidance, navigation and control in complex 3D surroundings including urban environments.
\n
\n2- Sensor fusion techniques that synthesize vision data and onboard micro-scale sensors that may include micro GPS processors, gyros, accelerometers, inclinometers, and speed sensors
\n
\n3- Agile, autonomous closed-loop control methodologies that utilize rapid path planning updates based on vision-derived information and mission requirements.
\n
\n4- Agile, autonomous closed-loop control methodologies that explore the tradeoff between vehicle control bandwidth, vision processing algorithm bandwidth, and the resulting stability of the coupled, closed loop system.
\n
\nThe Digital Worlds Institute's former Associate Director, Andy Quay, designed and built the virtual environment hardware systems. The display system located on the UF campus consists of three rear-projected video screens and a PC cluster for image generation. The center screen supports passive stereoscopic (polarized eyewear) for use with additional scientific visualization applications. The projection screen from <a href=http://www.stewartfilm.com/ target=_blank rel=noopener noreferrer>Stewart Filmscreen Corporation</a>, the <a href=http://www.projectiondesign.com/ target=_blank rel=noopener noreferrer>Projection Design</a> F1 SXGA video projectors, and the <a href=http://www.cyviz.com/ target=_blank rel=noopener noreferrer>CYVIZ</a> projector stacking mounts were installed by VizEveryWhere. The display system located at the REEF site consists of six 52 plasma screens. A dedicated server houses the 3D urban databases which is networked with a ten channel PC cluster for image generation. Dell sourced the computing resources for both facilities.
\n
\nThe virtual environment software is built upon <a href=http://www.multigenparadigm.com/ target=_blank rel=noopener noreferrer>MultiGen-Paradigm's</a> distributed VEGA, which allows the loading of the urban databases and synchronizes the video screens driven by the PC image clusters. Digital Worlds built one of the urban databases which spans several square miles.
\n
\nThere are now a total of four compatible visualization systems at UF, three on campus and one at the REEF. Two of the systems are housed in Digital Worlds' own research labs, the REVE and SAGE. All of the systems allow other types of scientific and engineering visualization as well.
\n<h4>Mechanical and Aerospace Engineering</h4>
\nDesign and implementation of Hardware in the Loop simulation of Autonomous Micro-Aerial Vehicles (MAV)
\n<h4>Digital Worlds Institute</h4>
\nDesign and implementation of virtual environment system to allow testing of MAV design in nondestructive Virtual Space]]></Description>
</project>

<project>
<Name>Museum Biomedical Engineering Kiosk</Name>
<Description><![CDATA[[embed]https://vimeo.com/44734836[/embed]
\n
\nA computer kiosk for the Museum of Science and Industry presenting the pioneering knee replacement research of UF's Dr. Fregly. Museum visitors learned how to collect data from various scientific machines, analyze the data using scientific visualization techniques, formulate an educated diagnosis of the patient's knee problem, and simulate surgical solutions.
\n
\nThe Biomedical Engineering Kiosk is a joint project between the Museum of Science and Industry (MOSI) in Tampa, Florida, the Digital Worlds Institute, and Dr. B.J. Fregly, a University of Florida Biomechanical Engineer and Professor. After receiving his NSF Career Grant, Dr. Fregly contacted the Digital Worlds Institute to present his pioneering knee replacement research to museum visitors in a language and level appropriate for children and adults.
\n
\nThe kiosk software allows visitors to play the role of a rising biomedical engineer. Museum visitors learn how to collect data from various scientific machines, analyze the data using scientific visualization techniques, formulate an educated diagnosis of the patient's knee problem, and simulate surgical solutions. The kiosk interface consists of real lab photo-based virtual environments, medical scanner simulators, 2D and 3D scientific visualization engines, and a what-if surgical simulator.]]></Description>
</project>

<project>
<Name>Music Instinct Project</Name>
<Description><![CDATA[The University of Florida’s WUFT was chosen as one of the ten PBS stations nationwide for the Music Instinct grant, working in partnership with the Digital Worlds Institute. Research shows that music has enormous potential to help explore the complexities of human brain function. For example, there’s a strong connection between the auditory and motor regions of the brain, and music seems to engage the motor system in a way that other modalities do not. People with motor disorders like Parkinson’s disease have improved their ability to walk while listening to a rhythm track, and stroke patients who have trouble with speech show signs of improvement when they receive music therapy. And there’s new evidence that music can actually change the physical structure of the brain – a fact that has critical implications for both education and medicine. One thing is clear, proven and agreed upon; music has a profound capacity to influence and alter the human experience.  Major funding for this program provided by the National Science Foundation.]]></Description>
<URL>http://www.pbs.org/wnet/musicinstinct/about/</URL>
</project>

<project>
<Name>Neuroprosthetic Training System</Name>
<Description><![CDATA[<header>
\n<h3>Neuroprosthetic Training System</h3>
\n</header>
\n<div class=full>
\n
\nWe are taking a multidisciplinary approach that combines existing and emerging techniques and technologies from Virtual Reality (VR) with Brain-Machine Interfaces to develop Virtual World Environments (VWE) as a training medium to enhance task improvement, stimulate new brain function, and provide a seamless learning transfer from a VR to a real environment. The science developed in this project will lead to a Neuroprosthetic Training System (NETS) that uses Virtual Reality to Treat Paralysis. To create this novel neuroprosthetic rehabilitation system, this project assembles a multidisciplinary team and environment in 4 areas: interactive digital media, biomedical engineering, neuroscience, and rehabilitation.
\n
\nPI: <strong>James C. Oliverio</strong>, Professor of Digital Media; Director, Digital Worlds Institute
\nCo-PI: <strong>Justin C. Sanchez</strong> Assistant Professor of Pediatrics, Neuroscience, Biomedical Engineering
\nCo-PI: <strong>Jose C. Principe </strong>Distinguished Professor of Electrical and Biomedical Engineering
\nCo-PI: <strong>Jill Sonke-Henderson</strong>, Director, Center for the Arts in Healthcare; Research Director OTRI
\n<h4>Abstract from funded NETS Project Proposal</h4>
\nMore than 2 million individuals in the U.S. suffer from a wide variety of neurological disorders that include spinal cord injury and diseases of the peripheral nervous system. While the symptoms and causes of these disabilities are diverse, there are at least two common characteristics in many of these neurological conditions: functioning of the brain remains intact, and the condition has a profoundly negative effect on the patient’s quality of life. New technologies called Brain-Machine Interfaces (BMI) offer an alternative means of communication and control that can bypass affected pathways of motor function through a direct interface with the brain. Many studies in animals and humans have shown the feasibility of closed-loop neural control, which allow the patient to use their “thoughts” to control prosthetic devices; however, they are often tested in specialized laboratory environments that do not translate well to the real world. <em>While functional proof-of-concept <strong>has been shown</strong>, the impact of BMI in the area of rehabilitation has yet to be realized because they have not been studied in the most appropriate environments where sensory and motor actions can be used to enhance performance. </em>Without a realistic and highly instrumented environment to train the user of the BMI, it is not possible to provide real-time performance feedback in the activities of daily life, graduated exposure to new context specific stimuli, augmented attention, or stimulating motivation for the user. <strong>The development of a new more appropriate environmental context for BMI could revolutionize rehabilitation by providing innovative ways to elicit new brain function that restores meaningful motor ability to disabled patients.</strong>
\n
\nTo overcome the noted challenges, we propose a multidisciplinary approach that combines existing and emerging techniques and technologies from Virtual Reality (VR) with Brain-Machine Interfaces to develop Virtual World Environments (VWE) as a training medium to enhance task improvement, stimulate new brain function, and provide a seamless learning transfer from a VR to a real environment. The science developed in this project will lead to a Neuroprosthetic Training System (NETS) that uses Virtual Reality to Treat Paralysis.  NETS would improve recovery by retraining the nervous system to improve motor function and reduce secondary conditions that impair physical or cognitive function. This program of research is enabled through Interactive Digital Media (IDM) coupled with a neuroprosthetic device that operates in real-time and is deployed through direct interaction with the brain. To create this novel <em>neuroprosthetic rehabilitation system</em>, this project assembles a multidisciplinary team in 3 research areas: interactive digital media, biomedical engineering, and neuroscience. This project builds upon the complimentary skillsets of the investigators to create a new and highly interdisciplinary collaborative partnership.
\n
\nThrough the formation of the Neuroprosthesis Program and Workshop in the past eleven years, the National Institutes of Health (NIH) National Institute of Neurological Disorders and Stroke (NINDS) has identified <em>neuroprosthetic rehabilitation</em> as a potential emergent area that is likely to impact the future of rehabilitation. Other significant sources of continued funding include the NSF, the DoD, the NIH, the Veterans Administration, as well as the prosthetics industry and medical technologies industries at large.
\n
\nIn summary, the impact of BMI in the area of rehabilitation has yet to be realized because they have not been studied in the most appropriate environments where sensory and motor actions can be used to enhance performance: <em>Virtual World Environments</em>. Without a rich interactive environment, the time needed to learn to control can be long and the level of mastery of function can be diminished. The development of the NETS rehabilitation system to reduce the time that it takes for a user to reach a specified performance level would be profoundly enabling to the millions of patients worldwide who currently suffer from spinal cord injury and diseases of the peripheral nervous system.
\n
\n</div>]]></Description>
</project>

<project>
<Name>Non Divisi</Name>
<Description><![CDATA[<h4>Non Divisi, an International Distributed Collaboration in the Performing Arts</h4>
\nOn October 15, 2003 the University of Florida's Digital Worlds Institute offered two performance/demonstrations of the real-time distributed collaboration Non Divisi (a musical term meaning not divided) joining co-creators from three continents at the Indiana State Museum for Internet2 Fall 2003.
\n
\nDancers in Korea rehearsed and performed with musicians, dancers and engineers in North and South America to demonstrate how telepresence can effectively empower multi-national collaborations in the performing arts. These performances were part of the ongoing work of Digital Worlds in growing an international network of Access Grid nodes and Internet2 members interested in creating new works of multi-national music and dance. Other partners included <a href=http://www.kaist.edu/ target=_blank rel=noopener noreferrer>Korean Advanced Institute of Technology (KAIST)</a>, <a href=http://www.sejong.edu/ target=_blank rel=noopener noreferrer>Sejong University</a>, <a href=http://www.reuna.cl/ target=_blank rel=noopener noreferrer>Red Universitaria Nacional (REUNA)</a>, and <a href=http://www.mdc.edu/nwsa/ target=_blank rel=noopener noreferrer>New World School of the Arts (NWSA)</a> with major support from Staff at <a href=http://www.internet2.org/ target=_blank rel=noopener noreferrer>Internet2</a>.]]></Description>
</project>

<project>
<Name>One Hand Clapping</Name>
<Description><![CDATA[<h4>World Premiere Performances of *One Hand Clapping*</h4>
\nDancers from Miami’s <a href=http://www.mdc.edu/nwsa/ target=_blank rel=noopener noreferrer>New World School of the Arts</a> and Musicians from the <a href=http://www.arts.ufl.edu/music/ target=_blank rel=noopener noreferrer>University of Florida’s School of Music</a> opened the Merce in Miami festivities with the premiere performances of the site-specific piece *One Hand Clapping* in the Ziff Ballet Opera House’s Arthur F. and Alice E. Adams Foundation lobby.
\n
\nThe large-scale environmental event was created by New World Choreographer Dale Andree in collaboration with the UF Digital Worlds Institute’s multi-media composer James Oliverio.]]></Description>
</project>

<project>
<Name>Pangea and the Age of the Dinosaurs</Name>
<Description><![CDATA[]]></Description>
</project>

<project>
<Name>Pruitt Sculpture</Name>
<Description><![CDATA[<h4>Sculpture for the J. Crayton Pruitt family Department of Biomedical Engineering</h4>
\nDigital Worlds created and managed an interdisciplinary team of artists and designers to create an original sculpture given to Dr. J. Crayton Pruitt. The sculpture was presented during a ceremony announcing his major gift of $10 million to the UF College of Engineering’s <a href=http://www.bme.ufl.edu/ target=_blank rel=noopener noreferrer>Biomedical Engineering Department</a>. DW artist in residence Arturo Sinclair worked with UF sculptor Brad Smith to create this distinctive work of alabaster and surgical vanadium steel.
\n
\nPruitt's gift is among the largest cash gifts received by UF. It is eligible for matching funds from the State of Florida Major Gift Trust Fund, which could result in a $20 million endowment for the newly named J. Crayton Pruitt Family Department of Biomedical Engineering.]]></Description>
</project>

<project>
<Name>RISK</Name>
<Description><![CDATA[<h4>Using Digital Technology for At-Risk youth</h4>
\nDrug and alcohol use are consistently associated with the occurrence of sexually transmitted diseases (STDs), including HIV. Despite this link, interventions designed to reduce substance use are rarely implemented in clinical settings where the prevalence of STDs and HIV is high (e.g., STD clinics). Brief interventions have proven effective in reducing hazardous drinking and drug use; however they are rarely integrated into clinical practice due to barriers at individual and organizational levels. We proposed that the ideal intervention should incorporate the successful components of brief motivational interviewing and should make use of interactive digital media and game features to enhance interest and acceptability, and should be easily adaptable within clinical settings.]]></Description>
</project>

<project>
<Name>SciPath</Name>
<Description><![CDATA[[embed]https://vimeo.com/44670105[/embed]
\n<h4>Exploring the Frontiers with UF Scientists</h4>
\nA series of short videos featuring accomplished professors of various areas of science (and their current University students) were created to inform and motivate young students in elementary and middle school to take an interest in careers in science.]]></Description>
</project>

<project>
<Name>Second Life College Fair 2008</Name>
<Description><![CDATA[Thanks to today’s rapidly evolving digital landscape, the ability to transcend traditional boundaries over the Internet is increasingly being utilized for not only social and business purposes but for educational advancement as well. Realizing this, the online world of Second Life hosted a virtual College Fair on November 16, 2008 that gave colleges and universities around the world the opportunity to promote their digital programs in a single shared virtual venue.
\n
\nThis College Fair allotted each of the participating institutions a parcel of virtual real estate for a “booth” to display its digital media program.  To take advantage of this globally accessible online showcase, the University of Florida (UF) Digital Worlds Institute assembled an interdisciplinary team of staff and students to create an interactive presence for UF on Second Life’s Information Island.
\n
\nCreating the booth for the College Fair required the same kind of interdisciplinary collaboration that fuels Digital Worlds’ research and development projects, and underlies the philosophy of its Master of Arts in Digital Arts and Sciences (DAS) degree.
\n
\nThe Digital Worlds (DW) team members came from diverse academic backgrounds and experiences, yet were able to work together effectively over the course of just a few weeks to create the interactive virtual booth, which featured video-on-demand and interactive posters.
\n
\nThe first team member was Lisa Hope, DW’s Digital Media Specialist, a UF graduate with a B.S. in Journalism and Communications, specializing in Online Media.  Her passion for writing and moviemaking led her to pursue interactive digital media; and her background in marketing and production, news and graphic design, and web-based media made her an ideal candidate for her position at Digital Worlds.
\n
\nIn contrast, Christian Tassin, Project Manager at DW, has a B.A. in English Literature and a M.S. in Entrepreneurship from the Warrington College of Business.  Tassin was attracted to the development of new technologies and how they evolve from university labs to marketable products. Before coming to DW, he wrote a business plan as part of a program called Integrated Technology Venture (ITV), where business and engineering students, professors, and industry professionals created a virtual company around patented UF inventions.  A recommendation from his ITV mentor led to his present position at Digital Worlds.
\n
\nMr. Gabriel Munoz-Calene is a second-year law student at UF with a B.A. from NYU, where he participated in the study Video as a Tool for Social Change.  He has great enthusiasm for digital video and virtual content and is also very interested in the use of digital media tools to provide a competitive advantage in the global marketplace.  His summer job with DW was extended into this school year as he continues to assist with research and the development of digital content.
\n
\nLeading the design of the UF booth was Joshua Javaheri, who has also worked as a professional video game developer at Ignition Entertainment Studios in Gainesville.  His strong interest in interactive technologies provided the catalyst that propelled him to pursue a career in computer entertainment and sciences.  He is currently a fifth-year DAS Engineering major who became involved with DW while working on his senior project: UF's First Virtual Campus Environment.
\n
\nAll four team members actually view their differences as an advantage that allows them to specialize in various aspects of a project, while at the same time applying their diverse skillsets to a common result, in this case the UF booth at the College Fair.  To them, interdisciplinary teams tend to create an exciting environment full of dynamic feedback. Chris Tassin put it best when he said, “Peers from different disciplines help you to gain new insights or expose you to new experiences that you may not have even known about.  People from divergent backgrounds can look at a problem from different perspectives and work together to create a better solution than one that could have been generated by a group of like-minded people.”
\n
\nWhether bridging the boundaries between academic disciplines, classrooms, continents or cultures, the UF Digital Worlds Institute remains dedicated to providing students an interdisciplinary platform upon which they can build their own career paths into the 21st Century.]]></Description>
</project>

<project>
<Name>The Faux Taxi</Name>
<Description><![CDATA[<h4>The Faux Taxi - Sound Image Light In Collaborative Arts</h4>
\nThe first emanation of ++silica entity took place April 3, 2003 at the 12th annual Florida Electro-acoustic Festival. This incarnation of ++silica entity, brainchild of Patrick Pagano, a graduate student in the Digital Arts and Sciences, featured a unique collaboration with undergraduate wunderkind, Adam Portnow, a senior in the Digital Arts and Sciences program. The presentation included immersive 5.1 digital audio paired with Access Grid technology to deliver high quality video from the Digital Worlds Institute laboratory in CISE 413 to the Phillips Center for the Performing Arts Black Box Theater in real time.
\n
\n++silica entity, the performance vehicle of SILICA, was an interdisciplinary ensemble of transmedia DAS sound &amp; video designers operating as a portable television show- AmbienTV. Real-time distributive performances were to be regarded as periodic episodics. Free in performing conception but directed by a shared pallet of audio and video source material, ++silica entity attempted to explore generative audio and video collaboration worldwide. Rehearsed by a shared structure process, then released into specified parameters to interact, ++silica entity existed only during performance, redefining 'improvisation' as it pertained to distributive &amp; collaborative digital works.]]></Description>
</project>

<project>
<Name>The Messenger</Name>
<Description><![CDATA[<h4>The Pittsburgh Symphony Orchestra presented The Messenger</h4>
\n<em>A Collaborative Work with new Music Visualization Technology</em>
\n
\n<strong>The Messenger</strong> was specially commissioned by the PSO for soloist <strong>Timothy Adams</strong>. It was a Concerto for Batterie (an old French term with no English equivalent, referring to both timpani and percussion instruments). The soloist actuated the images created by <strong>Steve Walker</strong> (Director of Visual Effects at Turner Studios in Atlanta) as he performed the new orchestral score by <strong>James Oliverio</strong> (Director of the <strong>Digital Worlds Institute</strong> at the <strong>University of Florida</strong>). The computer programming by <strong>Andy Quay</strong> (Associate Director, Digital Worlds ) utilized new music visualization techniques being developed at the <strong>Digital Worlds Institute</strong>.
\n
\n<strong>As Performed by The Pittsburgh Symphony Orchestra:</strong>
\n
\nJames Oliverio - <em>Music and Concept</em>
\nOsmo Vanska - <em>Conductor</em>
\nTimothy Adams - <em>Soloist</em>
\nAndy Quay - <em>Computer Programming</em>
\nSteve Walker - <em>Digital Visualization</em>]]></Description>
</project>

<project>
<Name>The Pigskin Professor</Name>
<Description><![CDATA[[embed]https://vimeo.com/44391398[/embed]
\n<h4>The Pigskin Professor, Engineering Around Us</h4>
\nProfessor Tony Schmitz combined his love of football with mechanical engineering in a series of episodes that showed how engineering pervades our daily lives. These videos, which were shown during the six home football games for the 2003 season, reached a diverse audience of over 84,000 fans at each game and provided a unique opportunity to discuss engineering in a high-profile forum. Using amusing classroom and gridiron-based scenarios, the Pigskin Professor cleverly demonstrated how engineering processes and results permeate our daily existence.
\n
\nThe highly entertaining episodes highlighted topics including: nanotechnology, light energy, heat transfer, and kinetic energy. Project principals included Tony Schmitz (writer and on-camera talent), Greg Sawyer (on-camera foil), Arturo Sinclair (director and animator), Marc Hoit (producer), Joella Walz (associate producer) and James Oliverio (executive producer).]]></Description>
</project>

<project>
<Name>Time to Fly Away</Name>
<Description><![CDATA[<h4></h4>
\n[embed]https://vimeo.com/44468125[/embed]
\n<h4>DW Produces Time to Fly Away: for all-College Commencement</h4>
\nTime to Fly Away was a tribute to the University of Florida Class of 2004. This five-minute digital media production premiered April 30th at the Convocation ceremony on Reitz Union Lawn for over 4,000 people. It included compelling interviews with students from the graduating class, original music by University students Lanae Hale and Gloria Castellon, and a live performance by Ms. Hale.*]]></Description>
</project>

<project>
<Name>Timpani Master Class</Name>
<Description><![CDATA[<h4>World's First Distributed Timpani Master Class</h4>
\nThe University of Florida's Digital Worlds Institute and Miami's New World Symphony co-hosted the world's first distributed timpani Master Class between the Atlanta Symphony and the New World Symphony. Both timpanists saw and heard each other over DVD quality video links with stereo CD quality audio in real time.
\n
\nThe University of Florida's Digital Worlds Institute and Miami's New World Symphony co-hosted the world's first distributed timpani Master Class between Atlanta Symphony timpanist Mark Yancich (located at the UF Digital Worlds Institute in Gainesville, Florida) and timpanist Alex Orfaly of the New World Symphony (located in Miami, Florida). The Master Class was based on performance preparations for Oliverio's Timpani Concerto #1 (The Olympian) with all audio and video transmitted over high-quality Internet2 during the session.
\n
\nBoth timpanists saw and heard each other over DVD quality video links with stereo CD quality audio in real time. Online observers around the world were also able to tune into a simultaneous webcast of the Master Class. UF percussion professor Dr. Ken Broadway and his students were present at the Digital Worlds REVE to observe the Master Class interaction.
\n<h4>Mark C. Yancich - Timpanist</h4>
\nPrincipal Timpanist of the Atlanta Symphony Orchestra since 1981, Mark Yancich is also active as a clinician, chamber musician, soloist, music publisher and at Emory University where he annually hosts the Mark Yancich Timpani Class each January, as well as The Atlanta Percussion (TAP) Seminar each June. He can be heard on over 95 recordings with the Atlanta Symphony, including most of the great choral/orchestral repertoire with Robert Shaw conducting. This is Mr. Yancich's first appearance at the University of Florida, Gainesville.
\n
\nMark is featured on the performance video of the recital version of James Oliverio's Timpani Concerto #1, and can also be seen on the Art of Timpani educational videos entitled, Changing and Tuning Plastic Timpani Heads with Mark Yancich, Tucking Calfskin Timpani Heads with Cloyd Duff, and the recently released (October 2003) video Sewing Felt Timpani Sticks.]]></Description>
</project>

<project>
<Name>Truth Be Told: Pirates</Name>
<Description><![CDATA[[embed]https://vimeo.com/64726337[/embed]]]></Description>
</project>

<project>
<Name>Virtual Enviroments for Therapeutic Solutions</Name>
<Description><![CDATA[<h4>Healing the Wounded Warrior</h4>
\n<ul>
\n 	<li><strong>Charles Levy, </strong>Chief of Physical Medicine and Rehabilitation, Malcolm Randall VA Medical Center (MRVAMC)</li>
\n 	<li><strong>James Oliverio, </strong>Director, Digital Worlds Institute (DW), Co-Director, Office for Transdisciplinary Research and Innovation (OTRI), University of Florida</li>
\n 	<li><strong>Jill Sonke, </strong>Director, Center for the Arts in Healthcare (CAHRE), Co-Director, Office for Transdisciplinary Research and Innovation (OTRI), University of Florida</li>
\n</ul>
\n<strong>Preamble</strong>
\nThrough a grant from the US Department of Commerce, the partners listed above have conceptualized and mobilized a major initiative to support returning Iraq war veterans through online virtual reality-based therapeutic, psychosocial, and vocational interventions.  The project focuses on therapeutic rehabilitation and re-integration of wounded warriors into their families, communities, and the workforce.
\n
\n<strong>Background and Rationale</strong>
\nA staggering 31% of soldiers returning from active duty in Afghanistan or Iraq and seen in veterans administration medical centers between 2001-2005 were diagnosed with mental health or psychosocial conditions1.  The prevalence of Post Traumatic Stress Disorder (PTSD) and Mild Traumatic Brain Injury (MTBI) in returning combat veterans is estimated at 20%, and the Defense and Veterans Brain Injury Coalition estimates that roughly 40% of injured warriors suffer from MTBI2.  It is estimated that a significant number of additional veterans suffering from such disorders go undiagnosed and untreated3.
\n
\nOften the most troubling symptoms of Post Traumatic Stress Disorder (PTSD) and Mild Traumatic Brain Injury (MTBI) are behavioral: mood changes, depression, anxiety, impulsiveness, emotional outbursts, intolerance of crowds, hyper-vigilance or inappropriate laughter. Other symptoms include disturbances in attention and memory, as well as delayed reaction time during problem-solving. Behavioral therapy, a mainstay of successful rehabilitation, generally requires that veterans be able to organize and negotiate daily activities, travel considerable distances, and interact with individuals in public to meet scheduled appointments - the very areas where impairments may be most profound. An additional barrier to rehabilitation for many returning combat soldiers is generational. Many younger warriors feel uncomfortable entering veterans administration medical centers, where both the patients and health care providers are older, and may be perceived as unsympathetic or unable to understand the issues of newly returning warriors.
\n
\nCurrently, there is not a widely recognized and effective treatment procedure to allow returning veterans to successfully re-integrate into the society they fight to defend. Yet there is compelling evidence that the use of virtual reality can contribute greatly to cognitive and affective rehabilitation and recovery4-6. A recent article (Oct. 6, 2007) on the front page of The Washington Post, entitled “Real Hope in Virtual Worlds: Online Identities Leave Limitations Behind”, describes how an increasing number of major health organizations are taking advantage of virtual worlds for public health education and patient support, and that many disabled individuals are “getting their lives back” through virtual world engagement7.
\n
\nVirtual World Environments (VWEs) hold great potential to solve many of the problems limiting effective treatment, support, and reintegration of wounded combat veterans into their families and into American society.  Internet-based VWEs are readily accessible from home at the user’s convenience. But unlike the public internet, the content of the VETS interaction can be carefully structured, controlled, and monitored by medical professionals. Many younger warriors are not only familiar with, but actually comfortable and fluent in, virtual realities through their exposure to electronic gaming and virtual combat training programs.  Although there are compelling precedents for the potential use of virtual reality-based scenarios8, not enough research and development has been undertaken to fully utilize today’s online virtual worlds to develop effective therapeutic interventions for veterans.
\n
\n<strong>Innovation</strong>
\nThis VETS initiative combines current digital technologies, clinical expertise, and focused testing to create an accessible and effective 21st century approach to the increasingly urgent problem of PTSD and MTBI in returning service-disabled veterans.
\n
\n<strong>Technology</strong>
\nVirtual World Environments are easily accessible, intuitive, and effective forms of virtual reality that provide secure real-time interaction between multiple users via the Internet. The VETS project will develop and test a novel online VWE platform that will serve as the basis for development and delivery of real-time interactive therapeutic treatment modalities. These readily accessible VWEs provide both psychosocial and psychoeducational interventions and vocational training for service disabled veterans suffering from PTSD/MTBI. The system will embody a unique capability to constantly enhance its efficacy based on user input, including user-designed content, clinical outcomes, and general use patterns. Thus, VWEs show great potential to augment or replace conventional treatment of PTSD/MTBI in returning combat veterans.
\n
\n<strong>Expertise</strong>
\nThe VETS project utilizes the expertise of a unique interdisciplinary team consisting of Malcolm Randall Veterans Administration Medical Center physicians specializing in physical medicine, rehabilitation, psychiatry, and psychology, as well as specialists from UF’s Digital Worlds Institute, Center for the Arts in Healthcare, the UF Health Sciences, and College of Engineering. This team will collaborate to ensure that the system platform and modalities are clinically, technically, and functionally relevant, and likewise appropriate, to the current treatment needs of wounded warriors.  UF’s Digital Worlds Institute, an international leader in digital technology innovation, will lead development of the system.
\n
\n<strong>Accessibility</strong>
\nVWE interventions can be accessed not only during traditional sessions in the offices of healthcare providers, but also from a computer in the privacy of warriors’ homes. In many cases this can reduce or even eliminated the need for recurring travel to a distant medical facility. They provide opportunities for interaction with others in a fully immersive, more controlled, and less threatening virtual environments, and capitalize on pre-existing technological skillsets possessed by many warriors in their 20s and 30s. VWEs can also provide veterans with 24-hour on-demand access to therapeutic modalities, as opposed to the ordinary business hours of healthcare workers. The potential applications of this system are far-reaching in this regard.
\n
\n<strong>Objectives</strong>
\nThe overarching aim of the VETS project is to build working prototypes of VWEs that are conceptually sound and ready for testing and refinement in clinical trials with combat veterans.  Three VWE applications will be modeled within this project:
\n<ul>
\n 	<li>Therapeutic Modalities that allow health professionals to interact with patients individually and in groups via the Internet.  The VWE will allow veterans to remotely participate in the creation of their own therapeutic environment using “avatars” (visual representations of themselves) with real-time text and audio communication. It is anticipated that this delivery platform will increase participation, accessibility, and acceptance of therapeutic interaction by warriors;</li>
\n 	<li>Therapeutic Simulations of ordinary social and vocational challenges to allow veterans to practice problem-solving in virtual real-life scenarios. Participants will be able to review the consequences of decisions, and get direct and immediate feedback regarding choices and responses. Unlike real life, they will have multiple opportunities to learn to deal with challenging situations with <em>no penalty for failure</em>; and</li>
\n 	<li>Vocational Tools and Creative Learning Modules for the development of digital media skillsets that are increasingly in demand for the workforce of the 21st Century. As the American job market moves from traditional service and manufacturing industries to digital and creativity-based professions, skills in 3D modeling, animation and graphical environments are increasingly sought after. One of the potential outcomes of VWE-assisted therapy is an increased fluency in both the tools and techniques of computer-based interactive media skillsets. We continue to develop our partnership with Navigator Development Group (National Disabled American Veterans (DAV) Small Employer of the Year 20079) and educational programs such as the Advanced Visualization and Interactive Design Center10 to develop and implement programs that are not only therapeutic but also accompanied by a significant vocational component.</li>
\n</ul>
\n<strong>Expected Outcome</strong>
\nThe proposed VWE-based system platform and modalities will effectively augment conventional treatment of PTSD/MTBI in returning combat veterans. It is scalable to regional, state and national levels and will significantly enhance our country's ability to deliver therapeutic and vocational support services to returning warriors as an appropriate reward and recognition of their tremendous sacrifice.
\n<h4>References Cited</h4>
\n<ol>
\n 	<li>Seal, K.H., Bertenthal, D., Miner, C.R., Sen, S., Marmar, C. (2007). Bringing the war back home: mental health disorders among 103,788 US veterans returning from Iraq and Afghanistan seen at Department of Veterans Affairs facilities.  Arch Intern Med., 167(5):476-82.</li>
\n 	<li>[No authors listed] (2006). The neurological burden of the war in Iraq and Afghanistan. Ann Neurol., 60(4):A13-5</li>
\n 	<li>Reeves R.R. (2007). Diagnosis and management of posttraumatic stress disorder in returning veterans. J Am Osteopath Assoc. 107(5):181-9.</li>
\n 	<li>Rose, F.D., Brooks, B.M. &amp; Rizzo, A.A. (2005). Virtual Reality in Brain Damage Rehabilitation: Review. CyberPsychology &amp; Behavior. 8(3):241-262.</li>
\n 	<li>Rothbaum BO, Hodges LF, Ready D, Graap K, Alarcon RD (2001). Virtual reality exposure therapy for Vietnam veterans with posttraumatic stress disorder.  J Clin Psychiatry, 62(8):617-22.</li>
\n 	<li>Josman N, Somer E, Reisberg A, Weiss PL, Garcia-Palacios A, Hoffman H. (2006). BusWorld: designing a virtual environment for post-traumatic stress disorder in Israel: a protocol. Cyberpsychol Behav., 9(2):241-4.</li>
\n 	<li>Stein, R. (2007). Real Hope in Virtual Worlds: Online Identities Leave Limitations Behind.  Washington Post.  Oct 6, 2007 edition, pg A01.</li>
\n 	<li>Halpern, S (2008). VIRTUAL IRAQ: Using simulation to treat a new generation of traumatized veterans. Annals of Psychology, The New Yorker. August 28, 2008. <a href=http://www.newyorker.com/reporting/2008/05/19/080519fa_fact_halpern?currentPage=all target=_blank rel=noopener noreferrer>http://www.newyorker.com/reporting/2008/05/19/080519fa_fact_halpern?currentPage=all</a></li>
\n 	<li><a href=http://www.ndgi.com/index.php?option=com_content&amp;view=article&amp;id=86&amp;Itemid=100 target=_blank rel=noopener noreferrer>http://www.ndgi.com/index.php?option=com_content&amp;view=article&amp;id=86&amp;Itemid=100</a></li>
\n 	<li><a href=http://www.i3dtrain.com/about.html target=_blank rel=noopener noreferrer>http://www.i3dtrain.com/about.html</a></li>
\n</ol>]]></Description>
</project>

<project>
<Name>Virtual Heritage</Name>
<Description><![CDATA[<h4>Extending Virtual Heritage Beyond the Local Site, Creating An International Digital Network</h4>
\nProfessor James Oliverio and DAS Graduate student Reeti Sompura presented a paper titled Extending Virtual Heritage Beyond the Local Site at the 9th International Conference on Virtual Systems and Multimedia (VSM 2003) in Montreal, Canada. The theme of the conference was “Hybrid Realities: Art, Technology and the Human Factor”.
\n
\n<strong>Abstract</strong>
\nBuilt heritage (i.e. Architecture) is said to be one of the finest forms of human endeavor. Perhaps the idea of capturing time in a tangible form first prompted people to build monuments in a quest for immortality. Given the passage of time, however, there has been significant decay and loss of these historic structures. It is crucial that we preserve our past so that current and future generations can share in our common heritage. But given the widespread geographic locations of cultural heritage sites throughout the world, it is doubtful that any one individual would be able to visit every one of them. Concurrently, even though digital technology is allowing teams around the globe to model (and thus preserve) a number of these sites, there is not a standard format in which these models are built and viewed.]]></Description>
</project>

<project>
<Name>Virtual Learning Forest</Name>
<Description><![CDATA[Virtual environments for experiential learning have many applications in sciences with laboratory-based training sessions. Some of the key advantages of such tools are: 1) cost-efficiency compared to real laboratories, 2) safety: the students can familiarize themselves with the proper laboratory safety procedures in a virtual setting before they go to an actual laboratory, and 3) accessibility, which is also important for distance education.
\n
\nFor forestry education, experiential learning systems show much potential for offering immersive experiences that simulate environmental areas including forests [Bec09], as well as virtual laboratory tools and processes [JBB<sup>+</sup>10] that simulate traditional real-world teaching and training methodologies. Such an educational interactive virtual reality system, named <em>Virtual Learning Forest</em>, was presented in [JBB<sup>+</sup>10] and offers visual simulation of longleaf pine ecosystems.
\n
\nThe target audience for Virtual Learning Forest is mainly undergraduate natural resource students. More specifically, the system has been tested by undergraduate forestry students in sampling, mensuration, and silviculture classes at the University of Florida and Virginia Polytechnic Institute and State University.  The students were involved with testing and evaluating the virtual environment during the development of various phases of this product. The fully-implemented version of a 3D Virtual Learning Forest based on the longleaf pine ecosystem was presented in [JBB<sup>+</sup>10]. In addition to the tridimensional simulation of this ecosystem, the virtual environment simulates several traditional tools and instruments for measuring the diameter of the trees (Fig. 3), the distance of the student from a specific tree, as well as the height of the longleaf pine trees. Finally, the system is connected to a database that records the progress of the students in various virtual laboratory exercises that simulate traditional laboratory methodologies and measure how students react to the virtual system and how this impacts their learning experience.
\n
\nThe target audience for Virtual Learning Forests will ultimately be undergraduate natural resource students at universities worldwide.  The immediate audience during the life of this project will be undergraduate forestry students in sampling, mensuration, and silviculture classes at the University of Florida (UF) and Virginia Polytechnic Institute and State University (Virginia Tech).  These students will be involved with testing and evaluating the product during the development of the first phase of this product. The goal during the 24 month life of this project will be to develop a first (<em>beta</em>) version of a 3D Virtual Learning Forest in a virtual world environment (VWE) based on the longleaf pine ecosystem, and to measure how students react to it and how it impacts their learning experience.]]></Description>
</project>

<project>
<Name>Voyage to Tomorrow</Name>
<Description><![CDATA[<h4>Voyage to Tomorrow, A Multi-media Suite in Three Movements</h4>
\nA three-movement symphonic suite performed live to an original digital movie created by the UF Digital Worlds Institute incorporating dramatic time-lapse cinematography, 3D computer animation and imagery from historic Renaissance paintings and sculptures.
\n
\n<strong>Voyage to Tomorrow<em> </em></strong>was specially commissioned for the kickoff of the University of Florida’s Capital Campaign, and received its Gainesville community world premiere with the School of Music Concert Choir, featuring six vocal soloists and an original digital media presentation. The innovative collaboration provided the grand finale for an exciting evening of live music that began at 7:30 p.m., Thursday, Oct. 18 in the University Memorial Auditorium.
\n
\n<strong>Voyage<em> </em></strong>was created by Digital Worlds Institute’s composer-in-residence James Oliverio, and featured a three-movement symphonic suite with live chorus and vocal soloists under the baton of conductor Will Kesling. It was performed live to an original digital movie created by the UF Digital Worlds Institute incorporating dramatic time-lapse cinematography, 3D computer animation and imagery from historic Renaissance paintings and sculptures.
\n
\n“The entire piece is a 21st century symphonic-scale composition, composed not only of the musical notes but also of the imagery, which was also designed to tell the story of mankind's long fascination with light as a metaphor for wisdom,” said James Oliverio, director of the Digital Worlds Institute.
\n
\nA special pre-concert dialogue with the composer and conductor was scheduled for 7 p.m. in the Auditorium. The public was welcomed and encouraged to attend.
\n
\n+ <a href=https://digitalworlds.ufl.edu/site/assets/files/91934/oct18pressrelease.pdf>Press Release</a>
\n
\n<strong>Digital Worlds Media Production Team</strong>
\n
\nJoella Wilson - <em>Compositor and Project Manager</em>
\nArturo Sinclair - <em>Digital Artist and Designer</em>
\nAndy Quay <em>- Technical Director</em>
\nLisa Hope - <em>Digital Media Specialist</em>
\nLindsay Amat - <em>Production Assistant</em>
\nSteve Walker - <em>Animation and Film</em>
\nWilliam VanDerKloot - <em>Time-lapse Cinematography</em>
\nBill Beckett<em> - Choral Recording Engineer</em>
\nKenneth Lovell - <em>Orchestral Recording Engineer</em>
\nJames Oliverio - <em>Composer and Project Director</em>
\nWill Kesling - <em>Conductor, UF Concert Choir</em>]]></Description>
</project>

<project>
<Name>VWE Working Group</Name>
<Description><![CDATA[<strong>Virtual World Environments (VWE) Working Group</strong>
\n
\nIn the Spring of 2008 a university-wide working group was convened to study the potential uses of Virtual World Environments (VWEs) in research and education at the University of Florida. Active participants came from nearly three-quarters of UF’s sixteen colleges and affiliated units. The group met three times over the course of the Spring semester at the Digital Worlds Institute’s Research, Education and Visualization Environment.
\n
\nEach of the Academic Deans was asked to recommend one person from their College to serve as their representative on the VWE Working Group. Additionally, Directors from a number of UF entities that showed an early interest in these technologies were also invited to send a representative. College representatives were given demonstrations of two existing VWE technologies: Linden Labs’ Second Life (SL) and Forterra Systems’ On-Line Interactive Virtual Environment (OLIVE).
\n
\nMany of the participants were not familiar with both of these VWEs, so both were demonstrated to afford all participants an equal opportunity to evaluate aspects of the various platforms. The participants were asked to speak with their colleagues across the departments of their respective colleges to ascertain levels of potential interest in the use of VWEs in their programs.
\n
\nThe initial reports of each of these representatives has been gathered and formatted herein. All of the images were captured from the two existing Second Life islands (referred to as the <a href=https://digitalworlds.ufl.edu/academics/digital-worlds-institute/research-projects-services/projects/research/gator-nation-island/>UF Islands</a>) constructed and currently maintained by the Digital Worlds Institute.
\n
\nAdditionally, the Spring 2008 section of the Interdisciplinary Research Seminar was asked to provide feedback on the use of VWEs from the student perspective. This diverse class consisted of nearly three dozen graduate students from across the UF campus, and a brief overview of their work and final projects for the semester is included herein. While we are at a relatively nascent stage in the awareness, acceptance and application of VWEs, numerous federal agencies, international corporations and universities are already exploiting the potential of this emergent suite of technologies.
\n
\nThe fact that so many of UF’s diverse colleges and units sent representatives to substantively participate in this effort is encouraging, and bodes well for UF’s potential adoption and implementation of these early 21st century opportunities.  As a result, the VWE Working Group moved forward with a Phase 2 in the 2008/09 academic year to further explore the potential for creative and collaborative use of these technologies at the University of Florida.]]></Description>
</project>

<project>
<Name>World House</Name>
<Description><![CDATA[[embed]https://vimeo.com/44465360[/embed]
\n<h4>WORLD HOUSE: Connecting the Global Community</h4>
\nOn the 40th anniversary of Martin Luther King Jr.’s assassination, the technology he lamented had overshadowed the human spirit was used to power four interactive global webcasts that transcend race, class, nation and religion.
\n
\nThe University of Florida’s Digital Worlds Institute in cooperation with King’s alma mater Morehouse College in Atlanta kicked off the first of four interactive global webcasts at 10 a.m. EDT on Friday, April 4, when experts from UF and Morehouse, along with institutions in China, India, Kenya and South Africa, discussed and shared in real-time King’s meaning for the 21st century, said James Oliverio, director of UF’s Digital Worlds Institute. The other three programs were also scheduled at 10 a.m. on successive Fridays in April, and all could be viewed at <a href=http://www.worldhouse.morehouse.edu/ target=_blank rel=noopener noreferrer>www.worldhouse.morehouse.edu</a>.
\n
\nIn his “World House” speech upon accepting the Nobel Peace Prize, King said “modern man has brought this whole world to an awe-inspiring threshold of the future. He has reached new and astonishing peaks of scientific success. He has produced machines that think….yet, in spite of these spectacular strides in science and technology, and still unlimited ones to come, something basic is missing. There is a sort of poverty of the spirit which stands in glaring contrast to our scientific and technological abundance.”
\n
\nThe UF-Morehouse international conversation used technology to bridge that divide, Oliverio said. “We’re going to have a cultural exchange, a scholarly dialogue and a motivational call to action for the students of today to carry forward the work of Dr. Martin Luther King Jr., who proceeded with the nonviolent resistance movement of Mahatma Gandhi in India and influenced Nelson Mandela in South Africa,” he said. “Using technology to literally connect places across the globe simultaneously, we will create a shared virtual space around the world on the network and have performances and workshops over that global platform.”
\n
\nThe outreach developed from a collaboration between UF and Morehouse College, the recipient of about 10,000 pieces of Martin Luther King Jr.’s personal writings in 2006. Terry Mills, a former UF dean who moved to Morehouse in 2007 to become the Margaret Mitchell Marsh Dean of Humanities and Social Sciences, said the idea came in discussions he had with Oliverio about how the two institutions might use the acquisition in educational programming.
\n
\nThe innovativeness of the technology at Digital Worlds Institute, which Mills called the “Imac Theater of Videoconferencing” for its ability to allow multiple partners around the globe to engage in an interactive, unified virtual space, made UF the natural choice to help produce the program, he said. “There are also geographic and historical reasons for the connection, notably Gainesville’s close proximity to St. Augustine where Dr. King had led freedom marches as well as its location near the site of the Rosewood massacre,” Mills said.
\n
\nThe purpose of the global discussions was not only to remind the world of King’s legacy but to keep his vision alive, as his message continues to have relevance today, Oliverio said.
\n
\n“This is a memorial to Dr. King, not just in the sense of looking backward to some academic papers in a museum, but honoring his life’s work in the hopes that students of today at Morehouse, UF and the other participating institutions will reassess their involvement with their own societies in the same way that Dr. King took a stand against oppression of African Americans in the United States,” he said. “Even at the beginning of the 21st-century human kind is still butchering each other in tribal conflicts over economic materialism and resources.”
\n
\nAlthough King’s “I Have a Dream” speech is well-known among college students, many are not familiar with the “World House” concept mentioned in his 1964 Nobel Peace Prize speech and his writings where he discusses the need to fight racism, war and poverty, he said.
\n
\nThe topic of the April 4th 90-minute session was King’s challenge to citizens in “transcending tribe, race, class, nation and religion to embrace the vision of World House.”  Speaker presentations as well as performances by artists, dancers and musicians were planned from each participating location, which, besides UF and Morehouse, included the U.S. Embassy in Beijing, China, the Maharaja Sayajirao University of Baroda in India; Kenyatta University in Nairobi, Kenya; and the U.S. Embassy in Johannesburg, South Africa. UF presenters from the Digital Worlds Institute’s Research, Education and Visualization Environment in 101 Norman Hall included Dr. Stephanie Evans, an African American studies and women’s studies professor, and drummer Mohamed DaCosta, a lecturer in UF’s College of Fine Arts School of Theatre and Dance. The 60-minute April 11 session featured UF social anthropologist Dr. Faye Harrison and poet Sharon Burney of UF’s African American Studies Program.]]></Description>
</project>

<project>
<Name>Virtual reality rehabilitation for ICU patients</Name>
<Description><![CDATA[<iframe src=https://www.youtube.com/embed/8A2_ScKBquM?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen data-mce-fragment=1></iframe>
\n
\nWhile virtual reality technology is gaining popularity in a variety of contexts from video games to fitting rooms, researchers at the University of Florida are discovering how VR can improve the quality of life of patients in the intensive care units of hospitals.
\n
\nUF Digital Worlds Institute Associate Director and Associate Professor Marko Suvajdzic and his team of researchers from digital arts and sciences, bioengineering, psychology and health have developed a VR technology project that is shown to alleviate the pain that ICU patients suffer.
\n
\nICU patients experience a variety of stresses and pains in the hospital, including isolation and witnessing tragedies. Moreover, their painkillers often trigger more stress and anxiety.
\n
\nAccording to Suvajdzic, previous studies have shown that meditation can be helpful in relieving stress. His team’s project, the Digital Rehabilitation Environment Augmenting Medical System (DREAMS), provides patients with guided meditations to help maintain their mental health in ICU.
\n<blockquote>“The idea behind DREAMS was to provide a more humane experience to the patients in the ICU unit in a hospital,” Suvajdzic said.</blockquote>
\nIn the experiment, patients used a VR headset to choose a scene they prefer and follow guided instructions to start the meditation. After various trials, Suvajdzic found that participants are most likely to choose the beach mode.
\n
\n“The sense of ‘presence’ has been transported from the reality of hospital to the virtual reality of this beach,” Suvajdzic said.
\n
\nBy doing so, the VR system takes patients out of the environment of the ICU that causes stress, anxiety and other negative effects on mental health.
\n
\nUnder the supervision of Suvajdzic and his colleague from the UF College of Medicine Dr. Azra Bihorac, the DREAMS team accomplished three trials by February 2019 with 60 ICU patients.
\n
\nBesides developing its own apps, the DREAMS team also incorporated some existing meditation apps into the system. In the initial exploratory trial started in early 2018, the team found that some video aspects of the system were not interactive enough.
\n
\n“Some of them were difficult to be interactive,” Suvajdzic said. “Some of them may have aspects that some patients didn’t like. So it really helped us hone in on the best system.
\n
\nSuvajdzic illustrated that an ideal version of the interactive VR system would employ gamification techniques that positively reinforce meditation behaviors for patients.
\n
\nOne example he gave was building a garden. At the beginning of the VR meditation, patients would see a garden with no plants. Every time patients adjust their breath to the level that the meditation recommends, the system would reward them by growing plants gradually.
\n
\nOnce patients finish the meditation and meet the breath control and relaxation goals, they would be able to see a beautiful garden in front of them, which Suvajdzic said could imply the rehabilitation of their bodies.
\n<blockquote>“In the way that the garden becomes a metaphor for their own body, as they get calm, their own body gets to feel better,” Suvajdzic said. “So it gives the person that visualization of the process that is happening internally to them as they are practicing meditation through virtual reality.”</blockquote>
\nThe DREAMS team is looking for larger, external grants to build a fully customizable app, in which it will create a more customized video to fit the environment of the ICU. In the future, when the system is developed enough, hospitals from all over the world would be able to purchase and implement it in their intensive care units.
\n
\nLooking ahead, Suvajdzic said researchers could potentially apply these ideas to designing and developing similar products for other situations, such as pediatrics and end-of-life care.
\n
\nSuvajdzic presented the DREAMS project as part of a talk he gave at Google in August about Emerging Technologies and Humanity.
\n<iframe src=https://www.youtube.com/embed/cSu37-UNP5k?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen data-mce-fragment=1></iframe>]]></Description>
</project>

<project>
<Name>3D Scanning the Rosetta Stone</Name>
<Description><![CDATA[Use 1-finger and 2-finger gestures to move, rotate, and zoom the 3D model of the Rosetta Stone below:
\n<iframe src=https://research.dwi.ufl.edu/op.n/file/2fzrs3i2cvas964f/embed width=1200px height=600px frameborder=0 scrolling=no></iframe>
\n
\nWith the permission of the British Museum, an interdisciplinary team from the University of Florida and the University of Leipzig scanned the Rosetta Stone in June 2018 to generate a high-resolution 2D and 3D map of its inscribed surface. In our setup, we used a single DSLR camera (Nikon D3400), which was fixed on a tripod in front of the stone, and calibrated as follows: exposure time = 5 sec., ISO speed = ISO-100, F-stop = f/25, focal length = 135mm, and max aperture = 4.5. To reconstruct the tridimensional inscribed surface using the shape-from-shading method, we controlled the lighting of the stone using a handheld light wand (Ice Light) that served as a 15-inch long light source of 1600 lumen at 5600k color temperature.
\n
\nWe divided the artifact in 8 regions (4 rows and 2 columns), which were photographed individually at 6000 x 4000 pixel resolution. Each region was photographed in 4 different lighting directions (light from the left, top, right, bottom) by placing the light wand in the corresponding side of the region of interest. This quadri-directional lighting configuration allowed us to capture information related to the local orientation at each point of the surface through the differences of the light reflection observed in the corresponding four photographs. The entire scanning session, including opening the glass case of the artifact, setting up the equipment, digitizing the artifact, and putting everything in its original configuration before the opening of the museum took us 120 min.
\n
\nDuring this time 32 photographs were taken in total (8 regions x 4 lighting conditions), which were then processed to compose high-resolution 2D and 3D representations of the surface with 0.08141mm sampling frequency, which is equivalent to 312 DPI resolution. The tridimensional details of the inscribed surface were captured in the depth map, which was computed by processing the four corresponding images of the same region of interest illuminated with four different lighting orientations using the method by A. Barmpoutis, E. Bozia, and R. Wagman published in the Journal of Machine Vision and Applications 21(6) in 2010. The depth map contains detailed three-dimensional information of the inscribed surface so that it can be visualized in 3D. The 3D reconstructed surface can be rendered as an interactive 3D model that can be manipulated by the user (move, scale, rotate) and can be inspected under different virtual lighting orientations and shading methods.
\n
\nFinally, in addition to the 3D reconstruction of the inscribed surface, we used a hand-held laser scanner (Structure Sensor by Occipital) mounted on a tablet computer (iPad Air by Apple) in order to create a 3D model of the entire stone. Although the 3D model generated by this scanner can depict the overall shape of the entire artifact, it does not have enough resolution to capture the fine details of the inscribed surface. Therefore, the 3D reconstructed surface using shape-from-shading is complementary to the laser-scanned 3D model, as both of these forms can co-exist in order to depict different structural details of the artifact.
\n
\nThe result of this process is a high resolution 3D representation of the Rosetta Stone that is available on-line as an interactive web app and can be accessed through the project's website.
\n
\nIn November 2019, the project was featured in German news outlets: <a href=https://www.mdr.de/wissen/stein-von-rosette-digital-leipzig-100.html>https://www.mdr.de/wissen/stein-von-rosette-digital-leipzig-100.html</a>]]></Description>
<URL>https://research.dwi.ufl.edu/projects/rosettastone</URL>
<Funding>UF College of the Arts Research Incentive Award\, 2018.</Funding>
</project>

<project>
<Name>Word Work Mat App for literacy instruction</Name>
<Description><![CDATA[<iframe src=https://www.youtube.com/embed/CPgzP6SXouk?feature=oembed width=1200 height=600 frameborder=0 allowfullscreen=allowfullscreen data-mce-fragment=1><span data-mce-type=bookmark style=display: inline-block; width: 0px; overflow: hidden; line-height: 0; class=mce_SELRES_start>﻿</span></iframe>
\n
\nDuring the COVID-19 pandemic, education has been severely impacted across the globe.
\nOngoing class sessions are especially important for K-12 students, many of whom benefit
\nespecially from experiential learning that is not typically offered in ad hoc online settings. UF
\nDigital Worlds Institute professor Angelos Barmpoutis is working in partnership with the UF
\nLiteracy Institute (UFLI) to address this world-wide need with an innovative virtual platform
\nsolution.
\n
\nThe Virtual Word Work Mat (VWWM) is an interactive app designed for literacy instruction,
\nbased on the original physical classroom version of the Word Work Mat created by UF doctoral
\nstudent Valentina Contesse. Valentina stated, “I never imagined when I was making Word Work
\nMats for my first graders, using file folders and Velcro, that it could be transformed into a
\ndigital tool that teachers all over the world would use in their classrooms!”
\n
\nAccording to Facebook analytics, just two days after its release date the VWWM had already
\nreached more than 70,000 people, with more than 6,000 engagements and 300 shares. And
\nUFLI’s online engagement increased by more than 1000 subscriptions during the same period.
\nCreating and offering ready access to the Virtual Word Work Mat during pandemic lockdown
\nhas empowered teachers and students continue their literacy instruction as part of their on-line
\nlearning activities. Designed to work on tablet and other devices using the either iOS or Android
\nplatforms, VWWM provides a simple user interface in which students can manipulate letter and
\nphoneme cards with intuitive touch gestures and compose words at home.
\n
\nHolly Lane, Director of UFLI said, “We’re so excited about the partnership between the UF
\nLiteracy Institute and Digital Worlds in our response to the pandemic. Thanks to the
\ncommitment and technical expertise of Angelos Barmpoutis, we were able to take some our
\ninteractive literacy instruction materials and bring them to life on a virtual platform. The
\nresponse from teachers has been overwhelmingly positive. We have many thousands of
\nteachers accessing the materials and sharing the links on social media.”
\n
\nThis excitement is shared by Digital Worlds Director James Oliverio. “One of the great benefits
\nof experiential online learning is accessibility across the traditional challenges of demographics,
\ngeography, and time zones. This project is an example of the interdisciplinary strengths of the
\nUniversity of Florida; faculty stepping up in a time of need to provide tangible benefits from the
\nongoing research and development happening across our campus.”
\n
\nUFLI Director Lane also stated, “Together, UFLI and DW are making a difference for teachers
\nand their students. We hope this is just the beginning of our collaboration!”
\n
\n<strong>Sample Comments from Parents and Teachers:</strong>
\n
\nI used this for the first time yesterday in my virtual reading lesson with my struggling readers.
\nTheir response was priceless. They were so engaged and actively asking me to change the
\nletters and make new words”
\n
\nThis has been amazing for my daughter with a severe Auditory Processing Disorder and
\ndysgraphia. Thank you!!!
\n
\nThis is amazing— thank you! Can’t wait to share with my teachers!
\n
\nThis is immeasurably helpful!!!!
\n
\n<strong>Press Release by the UF College of the Arts:</strong>
\n
\n<a href=https://arts.ufl.edu/in-the-loop/news/digital-worlds-institute-researcher-creates-experiential-online-learning-app/>https://arts.ufl.edu/in-the-loop/news/digital-worlds-institute-researcher-creates-experiential-online-learning-app/</a>
\n
\n<strong>Links to the App:</strong>
\n
\nTry the Virtual Word Work Mats below:
\n
\n<strong>Beginner Word Work Mat</strong>
\nDirect link: <a href=https://research.dwi.ufl.edu/op.n/file/cbhd8xmn9i4ctf7i/embed>Beginner Word Work Mat</a>
\nDrag and drop the letter cards below:
\n<iframe src=https://research.dwi.ufl.edu/op.n/file/cbhd8xmn9i4ctf7i/embed width=1200px height=600px frameborder=0 scrolling=no></iframe>
\n
\n<strong>Intermediate Word Work Mat</strong>
\nDirect link: <a href=https://research.dwi.ufl.edu/op.n/file/gc8nkxns914enc7d/embed>Intermediate Word Work Mat</a>.
\nDrag and drop the letter cards below:
\n<iframe src=https://research.dwi.ufl.edu/op.n/file/gc8nkxns914enc7d/embed width=1200px height=600px frameborder=0 scrolling=no></iframe>]]></Description>
<URL>https://education.ufl.edu/ufli/virtual-teaching/main/instructional-activities/decoding-and-encoding/word-work/</URL>
</project>

</projects>
</xml>